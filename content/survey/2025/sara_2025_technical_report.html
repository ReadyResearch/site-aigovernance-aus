<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr Michael Noetel">
<meta name="author" content="Dr Alexander Saeri">
<meta name="author" content="Jessica Graham">
<meta name="author" content="Dr Peter Slattery">

<title>Survey Assessing Risks from Artificial Intelligence (SARA) 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="sara_2025_technical_report_v2_files/libs/clipboard/clipboard.min.js"></script>
<script src="sara_2025_technical_report_v2_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="sara_2025_technical_report_v2_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="sara_2025_technical_report_v2_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="sara_2025_technical_report_v2_files/libs/quarto-html/popper.min.js"></script>
<script src="sara_2025_technical_report_v2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="sara_2025_technical_report_v2_files/libs/quarto-html/anchor.min.js"></script>
<link href="sara_2025_technical_report_v2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="sara_2025_technical_report_v2_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="sara_2025_technical_report_v2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="sara_2025_technical_report_v2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="sara_2025_technical_report_v2_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(report_styles/banner.png);
background-size: cover;
      }
</style>


<link rel="stylesheet" href="styles.css">
</head>

<body class="quarto-light">

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Survey Assessing Risks from Artificial Intelligence (SARA) 2025</h1>
            <p class="subtitle lead">Technical Report - Compiled on November 26, 2025</p>
                      </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-heading">Affiliations</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Dr Michael Noetel <a href="mailto:m.noetel@uq.edu.au" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              The University of Queensland
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Dr Alexander Saeri </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              The University of Queensland
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Jessica Graham </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              The University of Queensland
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Dr Peter Slattery </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              MIT FutureTech, Massachusetts Institute of Technology
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a>
  <ul class="collapse">
  <li><a href="#key-findings" id="toc-key-findings" class="nav-link" data-scroll-target="#key-findings">Key Findings</a></li>
  <li><a href="#recommendations" id="toc-recommendations" class="nav-link" data-scroll-target="#recommendations">Recommendations</a></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a></li>
  </ul></li>
  <li><a href="#findings-in-full" id="toc-findings-in-full" class="nav-link" data-scroll-target="#findings-in-full">Findings in Full</a>
  <ul class="collapse">
  <li><a href="#public-expectations-of-ai-governance" id="toc-public-expectations-of-ai-governance" class="nav-link" data-scroll-target="#public-expectations-of-ai-governance">Public Expectations of AI Governance</a>
  <ul class="collapse">
  <li><a href="#low-trust-inhibits-ai-adoption" id="toc-low-trust-inhibits-ai-adoption" class="nav-link" data-scroll-target="#low-trust-inhibits-ai-adoption">Low Trust Inhibits AI Adoption</a></li>
  <li><a href="#australians-dont-trust-tech-company-self-regulation" id="toc-australians-dont-trust-tech-company-self-regulation" class="nav-link" data-scroll-target="#australians-dont-trust-tech-company-self-regulation">Australians Don‚Äôt Trust Tech-Company Self-Regulation</a></li>
  <li><a href="#australians-worry-their-government-wont-go-far-enough" id="toc-australians-worry-their-government-wont-go-far-enough" class="nav-link" data-scroll-target="#australians-worry-their-government-wont-go-far-enough">Australians Worry Their Government Won‚Äôt Go Far Enough</a></li>
  <li><a href="#australians-think-regulation-is-lagging-behind-the-technology" id="toc-australians-think-regulation-is-lagging-behind-the-technology" class="nav-link" data-scroll-target="#australians-think-regulation-is-lagging-behind-the-technology">Australians Think Regulation is Lagging Behind the Technology</a></li>
  <li><a href="#australians-expect-the-government-to-prioritise-addressing-risks" id="toc-australians-expect-the-government-to-prioritise-addressing-risks" class="nav-link" data-scroll-target="#australians-expect-the-government-to-prioritise-addressing-risks">Australians Expect the Government to Prioritise Addressing Risks</a></li>
  </ul></li>
  <li><a href="#public-perceptions-of-ai-risk" id="toc-public-perceptions-of-ai-risk" class="nav-link" data-scroll-target="#public-perceptions-of-ai-risk">Public Perceptions of AI Risk</a>
  <ul class="collapse">
  <li><a href="#most-risks-are-seen-as-priorities" id="toc-most-risks-are-seen-as-priorities" class="nav-link" data-scroll-target="#most-risks-are-seen-as-priorities">Most Risks Are Seen As Priorities</a></li>
  <li><a href="#australians-are-increasingly-uncertain-about-ais-net-impact" id="toc-australians-are-increasingly-uncertain-about-ais-net-impact" class="nav-link" data-scroll-target="#australians-are-increasingly-uncertain-about-ais-net-impact">Australians Are Increasingly Uncertain About AI‚Äôs Net Impact</a></li>
  <li><a href="#australians-worry-about-losing-jobs-to-ai" id="toc-australians-worry-about-losing-jobs-to-ai" class="nav-link" data-scroll-target="#australians-worry-about-losing-jobs-to-ai">Australians Worry About Losing Jobs to AI</a></li>
  <li><a href="#australians-worry-about-losing-control-of-ai" id="toc-australians-worry-about-losing-control-of-ai" class="nav-link" data-scroll-target="#australians-worry-about-losing-control-of-ai">Australians Worry About Losing Control of AI</a></li>
  <li><a href="#most-mitigations-would-reportedly-increase-trust" id="toc-most-mitigations-would-reportedly-increase-trust" class="nav-link" data-scroll-target="#most-mitigations-would-reportedly-increase-trust">Most Mitigations Would Reportedly Increase Trust</a></li>
  </ul></li>
  <li><a href="#public-risk-tolerance" id="toc-public-risk-tolerance" class="nav-link" data-scroll-target="#public-risk-tolerance">Public Risk Tolerance</a>
  <ul class="collapse">
  <li><a href="#australians-expect-ai-to-be-as-safe-as-commercial-aviation" id="toc-australians-expect-ai-to-be-as-safe-as-commercial-aviation" class="nav-link" data-scroll-target="#australians-expect-ai-to-be-as-safe-as-commercial-aviation">Australians Expect AI To Be As Safe As Commercial Aviation</a></li>
  <li><a href="#even-radical-benefits-are-not-worth-catastrophic-risks" id="toc-even-radical-benefits-are-not-worth-catastrophic-risks" class="nav-link" data-scroll-target="#even-radical-benefits-are-not-worth-catastrophic-risks">Even Radical Benefits Are Not Worth Catastrophic Risks</a></li>
  <li><a href="#australians-show-patience-for-safety" id="toc-australians-show-patience-for-safety" class="nav-link" data-scroll-target="#australians-show-patience-for-safety">Australians Show Patience for Safety</a></li>
  <li><a href="#australians-are-divided-on-paying-for-safety" id="toc-australians-are-divided-on-paying-for-safety" class="nav-link" data-scroll-target="#australians-are-divided-on-paying-for-safety">Australians Are Divided on Paying for Safety</a></li>
  <li><a href="#support-for-global-ban-on-artificial-superintelligence" id="toc-support-for-global-ban-on-artificial-superintelligence" class="nav-link" data-scroll-target="#support-for-global-ban-on-artificial-superintelligence">Support for Global Ban on Artificial Superintelligence</a></li>
  <li><a href="#public-request-for-more-coverage" id="toc-public-request-for-more-coverage" class="nav-link" data-scroll-target="#public-request-for-more-coverage">Public Request for More Coverage</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#our-recommendations" id="toc-our-recommendations" class="nav-link" data-scroll-target="#our-recommendations">Our Recommendations</a>
  <ul class="collapse">
  <li><a href="#technology-specific-regulation-is-needed-to-meet-public-safety-expectations" id="toc-technology-specific-regulation-is-needed-to-meet-public-safety-expectations" class="nav-link" data-scroll-target="#technology-specific-regulation-is-needed-to-meet-public-safety-expectations">Technology-Specific Regulation Is Needed to Meet Public Safety Expectations</a></li>
  <li><a href="#technology-specific-regulation-would-likely-improve-safety-and-trust" id="toc-technology-specific-regulation-would-likely-improve-safety-and-trust" class="nav-link" data-scroll-target="#technology-specific-regulation-would-likely-improve-safety-and-trust">Technology-Specific Regulation Would Likely Improve Safety and Trust</a></li>
  </ul></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a>
  <ul class="collapse">
  <li><a href="#survey-design-and-implementation" id="toc-survey-design-and-implementation" class="nav-link" data-scroll-target="#survey-design-and-implementation">Survey Design and Implementation</a>
  <ul class="collapse">
  <li><a href="#sample-recruitment" id="toc-sample-recruitment" class="nav-link" data-scroll-target="#sample-recruitment">Sample Recruitment</a></li>
  <li><a href="#quality-control" id="toc-quality-control" class="nav-link" data-scroll-target="#quality-control">Quality Control</a></li>
  <li><a href="#randomisation" id="toc-randomisation" class="nav-link" data-scroll-target="#randomisation">Randomisation</a></li>
  </ul></li>
  <li><a href="#statistical-approach" id="toc-statistical-approach" class="nav-link" data-scroll-target="#statistical-approach">Statistical Approach</a>
  <ul class="collapse">
  <li><a href="#multilevel-regression-and-post-stratification-mrp" id="toc-multilevel-regression-and-post-stratification-mrp" class="nav-link" data-scroll-target="#multilevel-regression-and-post-stratification-mrp">Multilevel Regression and Post-stratification (MRP)</a></li>
  <li><a href="#technical-specifications" id="toc-technical-specifications" class="nav-link" data-scroll-target="#technical-specifications">Technical Specifications</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a>
  <ul class="collapse">
  <li><a href="#methodological-considerations" id="toc-methodological-considerations" class="nav-link" data-scroll-target="#methodological-considerations">Methodological Considerations</a>
  <ul class="collapse">
  <li><a href="#selection-bias" id="toc-selection-bias" class="nav-link" data-scroll-target="#selection-bias">Selection Bias</a></li>
  <li><a href="#attention-check-validity" id="toc-attention-check-validity" class="nav-link" data-scroll-target="#attention-check-validity">Attention Check Validity</a></li>
  <li><a href="#question-framing-effects" id="toc-question-framing-effects" class="nav-link" data-scroll-target="#question-framing-effects">Question Framing Effects</a></li>
  </ul></li>
  <li><a href="#implications-of-limitations" id="toc-implications-of-limitations" class="nav-link" data-scroll-target="#implications-of-limitations">Implications of Limitations</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#appendices" id="toc-appendices" class="nav-link" data-scroll-target="#appendices">Appendices</a>
  <ul class="collapse">
  <li><a href="#appendix-a-survey-questions" id="toc-appendix-a-survey-questions" class="nav-link" data-scroll-target="#appendix-a-survey-questions">Appendix A: Survey Questions</a>
  <ul class="collapse">
  <li><a href="#core-attitude-questions" id="toc-core-attitude-questions" class="nav-link" data-scroll-target="#core-attitude-questions">Core Attitude Questions</a></li>
  <li><a href="#trust-building-measures" id="toc-trust-building-measures" class="nav-link" data-scroll-target="#trust-building-measures">Trust-Building Measures</a></li>
  <li><a href="#risk-priorities" id="toc-risk-priorities" class="nav-link" data-scroll-target="#risk-priorities">Risk Priorities</a></li>
  <li><a href="#risk-tolerance-questions" id="toc-risk-tolerance-questions" class="nav-link" data-scroll-target="#risk-tolerance-questions">Risk Tolerance Questions</a></li>
  </ul></li>
  <li><a href="#appendix-b-full-label-plots" id="toc-appendix-b-full-label-plots" class="nav-link" data-scroll-target="#appendix-b-full-label-plots">Appendix B: Full-Label Plots</a></li>
  <li><a href="#appendix-c-statistical-details" id="toc-appendix-c-statistical-details" class="nav-link" data-scroll-target="#appendix-c-statistical-details">Appendix C: Statistical Details</a>
  <ul class="collapse">
  <li><a href="#model-specifications" id="toc-model-specifications" class="nav-link" data-scroll-target="#model-specifications">Model Specifications</a></li>
  <li><a href="#convergence-diagnostics" id="toc-convergence-diagnostics" class="nav-link" data-scroll-target="#convergence-diagnostics">Convergence Diagnostics</a></li>
  </ul></li>
  <li><a href="#appendix-d-sample-demographics" id="toc-appendix-d-sample-demographics" class="nav-link" data-scroll-target="#appendix-d-sample-demographics">Appendix D: Sample Demographics</a>
  <ul class="collapse">
  <li><a href="#sample-characteristics" id="toc-sample-characteristics" class="nav-link" data-scroll-target="#sample-characteristics">Sample Characteristics</a></li>
  </ul></li>
  <li><a href="#appendix-e-demographic-heterogeneity-analysis" id="toc-appendix-e-demographic-heterogeneity-analysis" class="nav-link" data-scroll-target="#appendix-e-demographic-heterogeneity-analysis">Appendix E: Demographic Heterogeneity Analysis</a>
  <ul class="collapse">
  <li><a href="#key-attitudes-by-age-group" id="toc-key-attitudes-by-age-group" class="nav-link" data-scroll-target="#key-attitudes-by-age-group">Key Attitudes by Age Group</a></li>
  <li><a href="#key-attitudes-by-education-level" id="toc-key-attitudes-by-education-level" class="nav-link" data-scroll-target="#key-attitudes-by-education-level">Key Attitudes by Education Level</a></li>
  </ul></li>
  <li><a href="#appendix-f-robustness-checks" id="toc-appendix-f-robustness-checks" class="nav-link" data-scroll-target="#appendix-f-robustness-checks">Appendix F: Robustness Checks</a>
  <ul class="collapse">
  <li><a href="#attention-check-performance" id="toc-attention-check-performance" class="nav-link" data-scroll-target="#attention-check-performance">Attention Check Performance</a></li>
  </ul></li>
  <li><a href="#sec-appendix-g" id="toc-sec-appendix-g" class="nav-link" data-scroll-target="#sec-appendix-g">Appendix G: Comparison Between Risk Tolerance and Forecast Risk Levels</a>
  <ul class="collapse">
  <li><a href="#setting-the-baseline-aviation-safety-standards" id="toc-setting-the-baseline-aviation-safety-standards" class="nav-link" data-scroll-target="#setting-the-baseline-aviation-safety-standards">Setting the Baseline: Aviation Safety Standards</a></li>
  <li><a href="#converting-xpt-forecasts-to-annual-per-person-risk" id="toc-converting-xpt-forecasts-to-annual-per-person-risk" class="nav-link" data-scroll-target="#converting-xpt-forecasts-to-annual-per-person-risk">Converting XPT Forecasts to Annual Per-Person Risk</a></li>
  <li><a href="#key-comparison" id="toc-key-comparison" class="nav-link" data-scroll-target="#key-comparison">Key Comparison</a></li>
  <li><a href="#contextualizing-the-gap" id="toc-contextualizing-the-gap" class="nav-link" data-scroll-target="#contextualizing-the-gap">Contextualizing the Gap</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#why-the-xpt-numbers-are-trustworthy-tournament-design-and-participant-credibility" id="toc-why-the-xpt-numbers-are-trustworthy-tournament-design-and-participant-credibility" class="nav-link" data-scroll-target="#why-the-xpt-numbers-are-trustworthy-tournament-design-and-participant-credibility">Why the XPT Numbers Are Trustworthy: Tournament Design and Participant Credibility</a></li>
  </ul></li>
  <li><a href="#sec-appendix-h" id="toc-sec-appendix-h" class="nav-link" data-scroll-target="#sec-appendix-h">Appendix H: Testing Robustness of Risk Tolerance and Scope Sensitivity</a></li>
  <li><a href="#appendix-i-risk-tolerance-among-consistency-validated-respondents" id="toc-appendix-i-risk-tolerance-among-consistency-validated-respondents" class="nav-link" data-scroll-target="#appendix-i-risk-tolerance-among-consistency-validated-respondents">Appendix I: Risk Tolerance Among Consistency-Validated Respondents</a>
  <ul class="collapse">
  <li><a href="#consistency-check-results" id="toc-consistency-check-results" class="nav-link" data-scroll-target="#consistency-check-results">Consistency Check Results</a></li>
  <li><a href="#casualty-risk-tolerance-consistency-validated" id="toc-casualty-risk-tolerance-consistency-validated" class="nav-link" data-scroll-target="#casualty-risk-tolerance-consistency-validated">Casualty Risk Tolerance (Consistency-Validated)</a></li>
  <li><a href="#risk-benefit-trade-offs-consistency-validated" id="toc-risk-benefit-trade-offs-consistency-validated" class="nav-link" data-scroll-target="#risk-benefit-trade-offs-consistency-validated">Risk-Benefit Trade-offs (Consistency-Validated)</a></li>
  <li><a href="#temporal-trade-offs-consistency-validated" id="toc-temporal-trade-offs-consistency-validated" class="nav-link" data-scroll-target="#temporal-trade-offs-consistency-validated">Temporal Trade-offs (Consistency-Validated)</a></li>
  <li><a href="#statistical-comparison" id="toc-statistical-comparison" class="nav-link" data-scroll-target="#statistical-comparison">Statistical Comparison</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="sara_2025_technical_report_final.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div><div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="sara_2025_technical_report_final.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div style="background-color: #E2F0F2; padding: 15px; margin-bottom: 20px; border-left: 4px solid #0C869B;">
<p style="margin: 0;"><strong>2025 Report</strong> | üìñ <a href="https://aigovernance.org.au/survey/">Survey overview</a> | üìÑ <a href="sara_2025_technical_report_final.pdf">PDF version</a> | üìä <a href="../2024/sara_technical_report.html">View 2024 Report</a></p>
</div>
<section id="executive-summary" class="level1">
<h1>Executive Summary</h1>
<p>While AI adoption is argued to be essential for economic competitiveness<span class="citation" data-cites="productivitycommission2025"><sup><a href="#ref-productivitycommission2025" role="doc-biblioref">1</a></sup></span>, there is public resistance driven by deep mistrust and perceptions of inadequate regulation<span class="citation" data-cites="gillespieTrustAttitudesUse2025"><sup><a href="#ref-gillespieTrustAttitudesUse2025" role="doc-biblioref">2</a></sup></span>. Our <a href="#method">survey</a> of 933 Australians finds the public expects AI to be as safe as commercial aviation - at least <strong>4,000x</strong> safer than current risk estimates. They want the government to better manage AI risks, and that many risk controls would increase their trust in AI.</p>
<section id="key-findings" class="level3">
<h3 class="anchored" data-anchor-id="key-findings">Key Findings</h3>
<ul>
<li><strong>Australians desire regulation:</strong> 74% worry the government <a href="#australians-worry-their-government-wont-go-far-enough">won‚Äôt regulate AI enough</a>, and 83% believe <a href="#australians-think-regulation-is-lagging-behind-the-technology">regulation is lagging behind</a> technological progress.<br>
</li>
<li><strong>Public trust in AI developers is extremely low:</strong> Less than one in four Australians (23%) <a href="#australians-dont-trust-tech-company-self-regulation">trust technology companies</a> to ensure AI safety.<br>
</li>
<li><strong>Risk management outweighs innovation:</strong> When forced to choose, 72% want the government to <a href="#australians-expect-the-government-to-prioritise-addressing-risks">prioritise managing AI risks</a> over driving innovation.<br>
</li>
<li><strong>Safety expectations as high as for aircraft:</strong> 94% expect AI systems to <a href="#australians-expect-ai-to-be-as-safe-as-commercial-aviation">meet or exceed the safety standards of commercial aviation</a>‚Äîaround 4,000 to 30,000 times safer than expert risk estimates.<br>
</li>
<li><strong>Australians reject catastrophic risks:</strong> Even a 1% <a href="#even-radical-benefits-are-not-worth-catastrophic-risks">chance of a global catastrophe</a> is considered unacceptable by most respondents.<br>
</li>
<li><strong>Patience for safety is widespread:</strong> A majority would support <a href="#australians-show-patience-for-safety">delaying advanced AI development</a> by up to 50 years if it reduced catastrophic risk from 5% to 0.5%.<br>
</li>
<li><strong>Australians want robust safety measures</strong>: More than 89% <a href="#most-mitigations-would-reportedly-increase-trust">would trust AI more</a> if there were mandatory safety testing, independent audits, or an Australian AI Safety Institute.<br>
</li>
<li><strong>Australians favour global limits on dangerous AI:</strong> 57% support an international treaty <a href="#support-for-global-ban-on-artificial-superintelligence">banning the development of ‚Äúsmarter-than-human‚Äù AI</a>.<br>
</li>
<li><strong>Demand for transparency and media coverage is strong</strong>: 80‚Äì85% want <a href="#public-request-for-more-coverage">more reporting</a> on AI‚Äôs societal effects and on how government is managing AI regulation.</li>
</ul>
</section>
<section id="recommendations" class="level3">
<h3 class="anchored" data-anchor-id="recommendations">Recommendations</h3>
<ul>
<li><strong>Adopt a precautionary approach to the most dangerous AI:</strong> Given strong support for patience and global cooperation, control development of high-risk systems <a href="https://superintelligence-statement.org/">until achieving credible safety evidence and public consensus</a>.</li>
<li><strong>Strengthen AI regulation and oversight:</strong> Establish enforceable safety standards, mandatory testing, and transparent reporting to close the gap between public expectations and current governance.</li>
<li><strong>Create an independent Australian AI Safety Institute:</strong> Coordinate safety research, conduct audits, and advise government‚Äîbuilding public trust while aligning with international best practices.</li>
</ul>
</section>
<section id="method" class="level3">
<h3 class="anchored" data-anchor-id="method">Method</h3>
<p>We recruited 1,068 Australian adults through an online panel between August 14-30, 2025, and analysed the 933 participants (87%) who passed two attention checks. We <a href="#methodology">used multilevel regression and post-stratification</a> (MRP)‚Äîthe current standard for converting non-probability samples into population estimates‚Äîto weight responses by age, gender, education, and state against 2021 Census distributions.</p>
<section id="declarations-of-conflict-of-interest" class="level4">
<h4 class="anchored" data-anchor-id="declarations-of-conflict-of-interest">Declarations of Conflict of Interest</h4>
<p>The authors have no conflicts to declare.</p>
</section>
<section id="citation" class="level4">
<h4 class="anchored" data-anchor-id="citation">Citation</h4>
<p>Noetel, M., Saeri, A.K., Graham, J.G., Slattery, P. (2025). <em>Survey Assessing Risks from Artificial Intelligence (SARA) 2025: Australian Public Attitudes Toward AI Risks and Governance.</em> The University of Queensland. https://aigovernance.org.au/survey/2025</p>
</section>
<section id="funding-declaration" class="level4">
<h4 class="anchored" data-anchor-id="funding-declaration">Funding Declaration</h4>
<p>This project was funded by the lead author.</p>
</section>
</section>
</section>
<section id="findings-in-full" class="level1">
<h1>Findings in Full</h1>
<section id="public-expectations-of-ai-governance" class="level2">
<h2 class="anchored" data-anchor-id="public-expectations-of-ai-governance">Public Expectations of AI Governance</h2>
<section id="low-trust-inhibits-ai-adoption" class="level3">
<h3 class="anchored" data-anchor-id="low-trust-inhibits-ai-adoption">Low Trust Inhibits AI Adoption</h3>
<p>Governments around the world argue that artificial intelligence is critical for economic growth and national security<span class="citation" data-cites="aiactionplan"><sup><a href="#ref-oecd2025" role="doc-biblioref">4</a></sup></span>. Yet Australia faces a fundamental challenge: public resistance threatens to undermine AI adoption as countries around the world pull ahead<span class="citation" data-cites="gillespieTrustAttitudesUse2025"><sup><a href="#ref-gillespieTrustAttitudesUse2025" role="doc-biblioref">2</a></sup></span>. In this report, we first explore why Australians struggle to trust AI and what regulations might increase that trust.</p>
<p>First, we explored their personal reasons for not using AI. When Australians avoid AI, they say it‚Äôs because they have privacy concerns (57%), because they prefer doing things without AI (39%), or because they don‚Äôt trust technology companies (32%) or like tech culture (31%).</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-barriers-adoption" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barriers-adoption-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/ai_usage_barriers_simple.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barriers-adoption-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Barriers preventing Australians from using AI tools
</figcaption>
</figure>
</div>
</div>
</div>
<p>This aligns with global findings where 57% of respondents agree AI poses significant threats to privacy<span class="citation" data-cites="internationalassociationofprivacyprofessionals2023"><sup><a href="#ref-internationalassociationofprivacyprofessionals2023" role="doc-biblioref">5</a></sup></span>, reflecting what Stanford HAI researchers call ‚Äúsocietal-level privacy risks that existing regulatory frameworks are not designed to address‚Äù<span class="citation" data-cites="kingRethinkingPrivacyAI2024"><sup><a href="#ref-kingRethinkingPrivacyAI2024" role="doc-biblioref">6</a></sup></span>.</p>
<p>Despite low rates of AI literacy<span class="citation" data-cites="gillespieTrustAttitudesUse2025"><sup><a href="#ref-gillespieTrustAttitudesUse2025" role="doc-biblioref">2</a></sup></span>, Australians do not report knowledge as a primary barrier<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This suggests that the public might be unlikely to engage with education around AI unless the core trust issues are resolved.</p>
</section>
<section id="australians-dont-trust-tech-company-self-regulation" class="level3">
<h3 class="anchored" data-anchor-id="australians-dont-trust-tech-company-self-regulation">Australians Don‚Äôt Trust Tech-Company Self-Regulation</h3>
<p>Currently, those developing or deploying AI in Australia are regulated by general laws designed for traditional technologies<span class="citation" data-cites="productivitycommission2025"><sup><a href="#ref-productivitycommission2025" role="doc-biblioref">1</a></sup></span> rather than a dedicated AI act, like that of the European Union<span class="citation" data-cites="europeanunionRegulationEU20242024"><sup><a href="#ref-europeanunionRegulationEU20242024" role="doc-biblioref">7</a></sup></span>. The Government has proposed voluntary guardrails that hope to align with international standards<span class="citation" data-cites="ausvoluntaryguardrails"><sup><a href="#ref-ausvoluntaryguardrails" role="doc-biblioref">8</a></sup></span>, but companies would be left to self-enforce these guardrails. The problem is: Australians don‚Äôt trust the organisations developing and deploying AI.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-trust-companies-deficit" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trust-companies-deficit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/trust_tech_companies_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trust-companies-deficit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Australian trust in tech companies to ensure AI safety
</figcaption>
</figure>
</div>
</div>
</div>
<p>When asked to what extent they trust tech companies to ensure the AI they develop is safe, <strong>77%</strong> of Australians said they trust them ‚Äúnot at all‚Äù or ‚Äúnot very much.‚Äù Only <strong>2%</strong> said they trust tech companies ‚Äúa great deal.‚Äù</p>
<p>Compared with an October 2023 international survey funded by the UK government<span class="citation" data-cites="deltapoll2023"><sup><a href="#ref-deltapoll2023" role="doc-biblioref">9</a></sup></span>, this places us among the least trusting of surveyed countries. Japan showed similar distrust of technology company self-regulation, which was substantially below Canada, USA, France, Italy, Singapore, the UK, and South Korea.</p>
<p>A 48,000-person study across 47 countries found systematically low confidence in commercial organisations developing AI in the public interest<span class="citation" data-cites="gillespieTrustAttitudesUse2025"><sup><a href="#ref-gillespieTrustAttitudesUse2025" role="doc-biblioref">2</a></sup></span>. That study also showed Australians had particularly low trust in those organisations (5th lowest of 47 countries). Theoretical frameworks suggest that this trust deficit stems from gaps between ethical principles and actual practices<span class="citation" data-cites="floridiAI4PeopleAnEthicalFramework2018"><sup><a href="#ref-floridiAI4PeopleAnEthicalFramework2018" role="doc-biblioref">10</a></sup></span>, or from governance failures<span class="citation" data-cites="winfieldEthicalGovernanceEssential2018"><sup><a href="#ref-winfieldEthicalGovernanceEssential2018" role="doc-biblioref">11</a></sup></span>.</p>
</section>
<section id="australians-worry-their-government-wont-go-far-enough" class="level3">
<h3 class="anchored" data-anchor-id="australians-worry-their-government-wont-go-far-enough">Australians Worry Their Government Won‚Äôt Go Far Enough</h3>
<p>Our data suggest the public are more concerned about the government failing to put in enough regulation, than regulating too much. When forced to choose, <strong>74%</strong> said they worry the government won‚Äôt regulate AI enough, while only <strong>26%</strong> fear over-regulation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-regulation-gap" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regulation-gap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/regulation_concern.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regulation-gap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Australian concerns about AI regulation pace
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="australians-think-regulation-is-lagging-behind-the-technology" class="level3">
<h3 class="anchored" data-anchor-id="australians-think-regulation-is-lagging-behind-the-technology">Australians Think Regulation is Lagging Behind the Technology</h3>
<p>The public also judge the regulatory response to be slow. Only 5% feel that regulation is developing ahead of the technology, and only 13% feel that regulation is keeping pace. 83% of Australians think regulation is falling behind technological innovation. This mirrors UK polling<span class="citation" data-cites="ipsos2023"><sup><a href="#ref-ipsos2023" role="doc-biblioref">12</a></sup></span> that shows only 11% of people think AI is keeping pace with AI technologies.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-regulation-pace" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regulation-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/regulation_pace_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regulation-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Perceived pace of AI regulation relative to technology development
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="australians-expect-the-government-to-prioritise-addressing-risks" class="level3">
<h3 class="anchored" data-anchor-id="australians-expect-the-government-to-prioritise-addressing-risks">Australians Expect the Government to Prioritise Addressing Risks</h3>
<p>The Federal Government<span class="citation" data-cites="productivitycommission2025"><sup><a href="#ref-productivitycommission2025" role="doc-biblioref">1</a></sup></span> describes tensions between promoting innovation and managing risks. When asked whether the government should focus on managing risks or driving innovation, <strong>72%</strong> of Australians said the government‚Äôs priority should be risk management. This preference aligns with the broader pattern: the public wants government to protect the public as AI is developed and deployed.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gov-priority" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gov-priority-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/government_priority.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gov-priority-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Public preferences for government AI priorities
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="public-perceptions-of-ai-risk" class="level2">
<h2 class="anchored" data-anchor-id="public-perceptions-of-ai-risk">Public Perceptions of AI Risk</h2>
<section id="most-risks-are-seen-as-priorities" class="level3">
<h3 class="anchored" data-anchor-id="most-risks-are-seen-as-priorities">Most Risks Are Seen As Priorities</h3>
<p>Australians did not have clear priorities for which risks they want the government to support. We drew risks from the international AI safety report<span class="citation" data-cites="ISRSAA2025"><sup><a href="#ref-ISRSAA2025" role="doc-biblioref">13</a></sup></span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> At the end, we also asked whether they felt the government should prioritise the risks on the public‚Äôs behalf, or prioritise economic adaptation following AI deployment.</p>
<p>Ten different risks showed strong agreement (&gt;87%), from privacy (96%) to cyber attacks (93%) and loss of control (89%). The public were less concerned about equitable global access to AI (53%) and energy use (73%). They were relatively unsupportive of experts focusing on adaptation (65% agreement; i.e., ‚Äòhelping the economy adapt to the change‚Äô) and expert prioritisation (76%).</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-risk-priorities" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-risk-priorities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/priority_risks.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:90.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-risk-priorities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: AI risks Australians believe government should prioritise
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="australians-are-increasingly-uncertain-about-ais-net-impact" class="level3">
<h3 class="anchored" data-anchor-id="australians-are-increasingly-uncertain-about-ais-net-impact">Australians Are Increasingly Uncertain About AI‚Äôs Net Impact</h3>
<p>When asked whether AI will do more good or harm overall, Australians in 2025 are essentially split three ways:</p>
<ul>
<li>33.3% believe AI will do more harm than good,</li>
<li>33.9% think harms will balance benefits, and</li>
<li>32.7% think it will do more good than harm.</li>
</ul>
<p>This represents a significant shift from 2024, when the public was more polarized. In our 2024 study<span class="citation" data-cites="saeri2024"><sup><a href="#ref-saeri2024" role="doc-biblioref">14</a></sup></span>, only 20% were neutral. This year, we saw a large increase in uncertainty (+13.9 percentage points), with corresponding decreases in both pessimistic (-9.7pp) and optimistic (-4.3pp) views.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-good-harm-shift" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-good-harm-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/ai_good_harm_ordinal.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-good-harm-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Growing uncertainty: Australians shift from polarized to ambivalent views on AI‚Äôs impact
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="australians-worry-about-losing-jobs-to-ai" class="level3">
<h3 class="anchored" data-anchor-id="australians-worry-about-losing-jobs-to-ai">Australians Worry About Losing Jobs to AI</h3>
<p>As a salient specific harm, we looked at concern about job loss. A recent Harvard study showed that companies that adopt AI have already reduced hiring for junior roles by 7.7% since 2023<span class="citation" data-cites="lichtinger2025"><sup><a href="#ref-lichtinger2025" role="doc-biblioref">15</a></sup></span>. When asked how worried they are that AI will lead to widespread unemployment, <strong>63%</strong> of Australians said they are ‚Äúfairly worried‚Äù or ‚Äúvery worried‚Äù about job loss from AI. This mirrors US data showing 56% of Americans are ‚Äòextremely or very concerned‚Äô about AI leading to job loss<span class="citation" data-cites="pew2024"><sup><a href="#ref-pew2024" role="doc-biblioref">16</a></sup></span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-worried-job-loss" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-worried-job-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/worried_job_loss_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-worried-job-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Level of worry about unemployment from AI
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="australians-worry-about-losing-control-of-ai" class="level3">
<h3 class="anchored" data-anchor-id="australians-worry-about-losing-control-of-ai">Australians Worry About Losing Control of AI</h3>
<p>As a more extreme risk, international AI experts worry<span class="citation" data-cites="ISRSAA2025"><sup><a href="#ref-ISRSAA2025" role="doc-biblioref">13</a></sup></span> that humanity could lose control of powerful frontier AI systems. When asked how worried they are that humans will lose control of AI, <strong>58%</strong> said they are ‚Äúfairly worried‚Äù or ‚Äúvery worried.‚Äù</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-worried-control" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-worried-control-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/worried_lose_control_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-worried-control-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Level of worry about humans losing control of AI
</figcaption>
</figure>
</div>
</div>
</div>
<p>These risks have different probabilities and severities<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, yet elicit similar levels of concern. This could reflect several factors:</p>
<ol type="1">
<li>Both widespread unemployment and loss-of-control represent fundamental disruptions to social order that people find deeply concerning regardless of probability.</li>
<li>The abstract nature of ‚Äòloss-of-control‚Äô<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> may lead people to underestimate its severity compared to the concrete, familiar threat of job loss, or</li>
<li>Australians may be implicitly weighting probability and severity, judging that near-certain economic disruption warrants similar concern as possible catastrophic outcomes.</li>
</ol>
</section>
<section id="most-mitigations-would-reportedly-increase-trust" class="level3">
<h3 class="anchored" data-anchor-id="most-mitigations-would-reportedly-increase-trust">Most Mitigations Would Reportedly Increase Trust</h3>
<p>We collected mitigations from various government proposals<span class="citation" data-cites="disr2024 senatorwiener2025 raise2025 seoul2024"><sup><a href="#ref-disr2024" role="doc-biblioref">19</a>‚Äì<a href="#ref-seoul2024" role="doc-biblioref">22</a></sup></span> to see if they would increase the public‚Äôs trust in AI.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>14 of the 15 mitigations we tested would make most Australians more likely to trust AI, with the right to human review (91%) and an Australian AI Safety Institute (90%) topping the list. Independent safety audits also attract overwhelming support (89%).</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-trust-building-main" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left" data-fig-pos="H">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trust-building-main-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/trust_measures.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:100.0%" data-fig-pos="H">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trust-building-main-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Measures that would increase Australian trust in AI
</figcaption>
</figure>
</div>
</div>
</div>
<p>Our estimates of population attitudes has some uncertainty (see confidence intervals on the right). As a result, it is hard to definitively rank the proposed measures using the data above. The strong support for most measures appears consistent with other polling. For example, pollsters have found between 60%<span class="citation" data-cites="aipicollab2024"><sup><a href="#ref-aipicollab2024" role="doc-biblioref">23</a></sup></span> and 80%<span class="citation" data-cites="aipisb1047"><sup><a href="#ref-aipisb1047" role="doc-biblioref">24</a></sup></span> of US voters support mandatory testing, safety protocols, and liability for frontier AI model developers.</p>
<p>These indicate that the government likely has many levers through which it could increase trust and safety.</p>
</section>
</section>
<section id="public-risk-tolerance" class="level2">
<h2 class="anchored" data-anchor-id="public-risk-tolerance">Public Risk Tolerance</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Safety Standards and Risk Tolerance
</div>
</div>
<div class="callout-body-container callout-body">
<p>In many areas of public safety (e.g., civil engineering, power generation, aviation), regulators set acceptable safety standards and certify engineers to design systems that meet those standards<span class="citation" data-cites="iso31000"><sup><a href="#ref-iso31000" role="doc-biblioref">25</a></sup></span>. In this model, the public and their elected representatives might not be consulted on the specific safeguards put in place. Instead, if they are consulted, regulators might instead mostly consider their risk tolerance. For example, they might not have input in the design of a dam, but could rightly expect the dam to have a very low chance of bursting. The regulator is expected to make that risk transparent so the public can make informed decisions (e.g., to move, or to request a lower risk tolerance).</p>
<p>For AI safety, a similar approach would be understand the levels of societal risk the public would be willing to tolerate, then let regulators put in place the safeguards that bring risk down to those levels. As noted above, it is still important to understand which safeguards would increase the public‚Äôs trust, if trust is important for increasing adoption. However, they are not technical experts who understand which mitigations actually reduce risk. For example, they might say that ‚Äòwatermarking‚Äô AI-generated content would increase their trust without understanding how easy it is to circumvent watermarks<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>If the goal is both real and perceived trust, it is important to understand stakeholder preferences <em>and</em> expert judgements about which mitigations reduce risk to tolerable levels. In this section, we outline the public‚Äôs expectations of those levels.</p>
</div>
</div>
<section id="australians-expect-ai-to-be-as-safe-as-commercial-aviation" class="level3">
<h3 class="anchored" data-anchor-id="australians-expect-ai-to-be-as-safe-as-commercial-aviation">Australians Expect AI To Be As Safe As Commercial Aviation</h3>
<p>We asked whether AI should have standards more or less strict than aviation. 94% of Australians want advanced AI to meet aviation-grade safety or better, including 58% who prefer standards stricter than airlines and just 6% willing to accept looser rules.</p>
<p>This is far safer than current estimates from forecasting and AI experts<span class="citation" data-cites="ezrakarger2023 graceThousandsAIAuthors2024"><sup><a href="#ref-ezrakarger2023" role="doc-biblioref">27</a>,<a href="#ref-graceThousandsAIAuthors2024" role="doc-biblioref">28</a></sup></span> who put the risk of 8 billion deaths between 1% and 10%. Leading AI researchers, and the <a href="https://youtu.be/rF0tQtDMwHM?si=lwVcPWGyG-AR26LB&amp;t=1461">CEOs</a> of <a href="https://www.axios.com/2025/09/17/anthropic-dario-amodei-p-doom-25-percent">frontier AI companies</a>, estimate catastrophic risks from advanced AI between 2% and 25%.</p>
<p>Even the most optimistic credible assessment (superforecasters) shows AI risk at <strong>~4,000x</strong> aviation levels. The median domain expert assessment suggests <strong>~30,000 times</strong> higher risk than aviation (see <a href="#appendix-g-comparison-between-risk-tolerance-and-forecast-risk-levels">Appendix G</a>).</p>
<p><strong>There is a gulf between expectations and reality:</strong> The public expects AI to match the safety of an industry that hasn‚Äôt had a major Australian commercial airline fatality, while experts, forecasters, and AI CEOs assess it as carrying extinction-level risks.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ai-airline" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ai-airline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/ai_vs_airline_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ai-airline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Desired AI safety standards compared to aviation
</figcaption>
</figure>
</div>
</div>
</div>
<p>We explored directly asking about risk tolerances for different sized risks (see <a href="#sec-appendix-h">Appendix H</a>). There were trade-offs between making the question simple and making small probabilities easy to understand. These probabilities are difficult to validly elicit from the public, but findings generally support claims above: people expect safety standards akin to commercial aviation.</p>
</section>
<section id="even-radical-benefits-are-not-worth-catastrophic-risks" class="level3">
<h3 class="anchored" data-anchor-id="even-radical-benefits-are-not-worth-catastrophic-risks">Even Radical Benefits Are Not Worth Catastrophic Risks</h3>
<p>We also tested whether major benefits could justify catastrophic risks. Each respondent was randomly assigned a risk level (0.01% to 20%) and asked: ‚ÄúAn advanced AI system has a [X]% chance of causing a global catastrophe (over 1 million deaths) but a 60% chance of solving climate change and extending human lifespan by 20 years. Is this trade-off acceptable?‚Äù</p>
<p>As seen below, even small probabilities of catastrophic outcomes are contentious for the public. No level of catastrophic risk is clearly acceptable.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-risk-benefit" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-risk-benefit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/risk_benefit_tradeoff.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-risk-benefit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Acceptability of catastrophic AI risks for major benefits
</figcaption>
</figure>
</div>
</div>
</div>
<p>At 0.01% risk (1 in 10,000), <strong>46% reject</strong> the trade-off. At 1% risk, <strong>51% find it unacceptable</strong>. This indicates the public are divided on whether AI that could ‚Äòsolve climate change and extend lifespan‚Äô is worth even a 1 in 1,000 risk of a catastrophe.</p>
</section>
<section id="australians-show-patience-for-safety" class="level3">
<h3 class="anchored" data-anchor-id="australians-show-patience-for-safety">Australians Show Patience for Safety</h3>
<p><strong>Question Design:</strong> We measured time preferences for AI safety by randomizing delay periods (1 to 50 years). Respondents were asked: ‚ÄúSuppose we could reduce the risk of AI catastrophe from 5% to 0.5% by delaying advanced AI development by [X] years. Would this delay be worthwhile?‚Äù</p>
<p>There was strong support for delays across all timeframes tested (57% to 87%). Most Australians (80%) would support a 10-year delay, and even <strong>50-year delays receive majority support</strong> (57%), with only 8% saying such delays were not worthwhile. Australians prioritise safety over speed, willing to wait generations for safer AI rather than accept 5% catastrophic risk.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-temporal-tradeoff" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-temporal-tradeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/temporal_tradeoff.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-temporal-tradeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Willingness to delay AI deployment for safety improvements
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="australians-are-divided-on-paying-for-safety" class="level3">
<h3 class="anchored" data-anchor-id="australians-are-divided-on-paying-for-safety">Australians Are Divided on Paying for Safety</h3>
<p>Our data show 22% of Australians say the safety improvements are <strong>not worth paying for</strong>, while a similar share (24%) say they <strong>cannot put a price</strong> on preventing catastrophe. About 40% would contribute a modest annual amount (up to $100), and only 14% are ready to pay more than $100 each year. Overall, roughly half of the public (54%) is willing to pay something to cut the risk from 1% to 0.01%, but contributions cluster at the lower end (23% would pay just $1‚Äì25, and 17% would pay $26‚Äì100).</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-willingness-pay" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-willingness-pay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/willingness_to_pay_ordinal.png" class="img-fluid quarto-figure quarto-figure-left figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-willingness-pay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Willingness to pay annually to reduce catastrophic AI risk from 1% to 0.01%. Bars show MRP-adjusted population estimates with 95% credible intervals. N=933 respondents who passed attention checks.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="support-for-global-ban-on-artificial-superintelligence" class="level3">
<h3 class="anchored" data-anchor-id="support-for-global-ban-on-artificial-superintelligence">Support for Global Ban on Artificial Superintelligence</h3>
<p>The International AI Safety Report<span class="citation" data-cites="bengioManagingAIRisks2024"><sup><a href="#ref-bengioManagingAIRisks2024" role="doc-biblioref">29</a></sup></span> suggests that substantial risk of catastrophic outcomes stems from difficulty controlling smarter-than-human systems. While this is a goal of many frontier AI companies (e.g., ChatGPT‚Äôs <a href="https://openai.com/index/planning-for-agi-and-beyond/">OpenAI</a>), <strong>57%</strong> of Australians support an international treaty banning the development of ‚Äòsmarter-than-human‚Äô artificial intelligence (a.k.a., ‚Äòartificial superintelligence‚Äô).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-treaty-ban" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-treaty-ban-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/treaty_ban_agi.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-treaty-ban-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Support for international treaty banning superintelligent AI
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="public-request-for-more-coverage" class="level3">
<h3 class="anchored" data-anchor-id="public-request-for-more-coverage">Public Request for More Coverage</h3>
<p>Beyond specific risks, Australians want ongoing public discourse about AI‚Äôs societal impacts and AI regulation. 79% of Australians want more media coverage of AI‚Äôs societal impacts and 85% want more reporting on how government is regulating AI; barely 21% and 15% say they do <strong>not</strong> need additional coverage.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-media-society" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-media-society-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/media_ai_society_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-media-society-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Desire for media coverage of AI‚Äôs societal impact
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-media-regulation" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-media-regulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/media_ai_regulation_ordinal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-media-regulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Desire for media coverage of AI regulation
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="our-recommendations" class="level1">
<h1>Our Recommendations</h1>
<section id="technology-specific-regulation-is-needed-to-meet-public-safety-expectations" class="level3">
<h3 class="anchored" data-anchor-id="technology-specific-regulation-is-needed-to-meet-public-safety-expectations">Technology-Specific Regulation Is Needed to Meet Public Safety Expectations</h3>
<p>Bodies like the Australian Productivity Commission argue<span class="citation" data-cites="productivitycommission2025"><sup><a href="#ref-productivitycommission2025" role="doc-biblioref">1</a></sup></span> that technology-specific regulations should be ‚Äòa last resort‚Äô because badly designed or heavy handed rules would slow adoption. Poorly coordinated regulations are cited as adoption headwinds for global firms<span class="citation" data-cites="oecd2025"><sup><a href="#ref-sloan2024" role="doc-biblioref">30</a></sup></span>. In our view, the Productivity Commission correctly identifies that Australia‚Äôs existing legal frameworks<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> provide substantial scaffolding for AI governance<span class="citation" data-cites="productivitycommission2025"><sup><a href="#ref-productivitycommission2025" role="doc-biblioref">1</a></sup></span>. Companies that steward data responsibly, ensure algorithmic fairness, and maintain transparency may build trust incrementally.</p>
<p>However, our findings suggest the public‚Äôs perceptions of slow or weak regulation might be inhibiting trust and adoption. For example:</p>
<ul>
<li>They fear that AI currently has <a href="#low-trust-inhibits-ai-adoption">insufficient privacy protections</a>, that the regulation is <a href="#australians-think-regulation-is-lagging-behind-the-technology">lagging behind the technology</a>, and think the government is <a href="#australians-worry-their-government-wont-go-far-enough">more likely to under- than over-regulate</a>.</li>
<li>They think the government should <a href="#australians-expect-the-government-to-prioritise-addressing-risks">prioritise risk management</a> over accelerating adoption, and <a href="#most-risks-are-seen-as-priorities">see many risks as priorities</a>. For example, 63% worry about <a href="#australians-worry-about-losing-jobs-to-ai">losing jobs</a> and 58% <a href="#australians-worry-about-losing-control-of-ai">losing control of AI itself</a>.</li>
<li>Australians expect stringent standards on AI, akin to the <a href="#australians-expect-ai-to-be-as-safe-as-commercial-aviation">commercial aviation industry</a>. <a href="#sec-appendix-g">Expert assessments</a> suggest risks over the coming decades are at least 4,000x higher than this expectation.</li>
<li>If it reduces risks, Australians are <a href="#australians-show-patience-for-safety">willing to wait for advanced AI</a>, and about <a href="#australians-are-divided-on-paying-for-safety">half seem willing to pay</a>.</li>
<li>They <a href="#public-request-for-more-coverage">want to hear more</a> about how AI is affecting society and what is being done to regulate it.</li>
</ul>
</section>
<section id="technology-specific-regulation-would-likely-improve-safety-and-trust" class="level3">
<h3 class="anchored" data-anchor-id="technology-specific-regulation-would-likely-improve-safety-and-trust">Technology-Specific Regulation Would Likely Improve Safety and Trust</h3>
<p>AI creates unprecedented risks<span class="citation" data-cites="bengioManagingAIRisks2024"><sup><a href="#ref-bengioManagingAIRisks2024" role="doc-biblioref">29</a></sup></span>. For example, leading AI companies plan to build ‚Äòagentic‚Äô general models ‚Äúthat can autonomously plan, act, and pursue its own goals across almost all tasks that humans can perform‚Äù<span class="citation" data-cites="bengio2025"><sup><a href="#ref-bengio2025" role="doc-biblioref">31</a></sup></span>. Capabilities like these pose catastrophic risks<span class="citation" data-cites="bengioManagingAIRisks2024"><sup><a href="#ref-bengioManagingAIRisks2024" role="doc-biblioref">29</a></sup></span>. The forecast risks from AI are much <a href="#safety-standards-and-risk-tolerance">higher than the public is willing to tolerate</a>. As a result, governments around the world are implementing a range of technology-specific safeguards<span class="citation" data-cites="europeanunionRegulationEU20242024"><sup><a href="#ref-europeanunionRegulationEU20242024" role="doc-biblioref">7</a></sup></span>.</p>
<p>Many such safeguards focus on catastrophic risks from frontier AI development<span class="citation" data-cites="senatorwiener2025"><sup><a href="#ref-senatorwiener2025" role="doc-biblioref">20</a></sup></span>. They focus on these risks due to the size of the threat and because more comprehensive regulation has been more controversial<span class="citation" data-cites="carnegiesb53"><sup><a href="#ref-carnegiesb53" role="doc-biblioref">32</a></sup></span>. The regulations aim to increase safety without imposing unnecessary standards on smaller companies and low-risk uses of AI<span class="citation" data-cites="ussc2025"><sup><a href="#ref-ussc2025" role="doc-biblioref">33</a></sup></span>. Our survey data shows that many mitigations would reportedly increase the public‚Äôs trust. For example, they say they would be more likely to trust AI if the government implemented SB53‚Äôs key provisions:</p>
<ul>
<li>Require incident reporting</li>
<li>Require published safety protocols</li>
<li>Protect whistleblowers from retaliation</li>
</ul>
<p>They also felt it would increase trust if the government had an AI Safety Institute to understand risk and work with industry, as they have in the <a href="https://www.aisi.gov.uk/">UK</a>, <a href="https://www.nist.gov/caisi">USA</a> and <a href="https://www.aisi.re.kr/eng">Korea</a>. These mitigations might be useful first steps that monitor and reduce risk while bringing Australia into line with emerging standards.</p>
<p>Instead of aiming to only meet emerging standards, Australia may need to look ahead and lead in some areas, particularly given how quickly AI is developing. As noted above, the public are more concerned about the government being too slow and doing too little. Our previous work<span class="citation" data-cites="saeri2024"><sup><a href="#ref-saeri2024" role="doc-biblioref">14</a></sup></span> showed 8 in 10 want Australia to lead in international governance of AI. Regulators could therefore consider the other popular mitigations (e.g., independent safety testing, emergency shutdown capabilities, developer liability for harms) argued to reduce catastrophic risks. There are emerging dialogues about which additional mitigations would be most effective amid rapid progress<span class="citation" data-cites="ISRSAA2025 bengioManagingAIRisks2024"><sup><a href="#ref-ISRSAA2025" role="doc-biblioref">13</a>,<a href="#ref-bengioManagingAIRisks2024" role="doc-biblioref">29</a></sup></span>.</p>
<p>There are also emerging dialogues around the need to pursue agentic general AI models at all<span class="citation" data-cites="superstatement"><sup><a href="#ref-superstatement" role="doc-biblioref">34</a></sup></span>: 25,000 signatories have called for ‚Äúa prohibition on the development of superintelligence, not lifted before there is broad scientific consensus that it will be done safely and controllably, and strong public buy-in.‚Äù We found majority support (57%) for an even more strict ‚Äúinternational treaty to ban any ‚Äòsmarter-than-human‚Äô AI‚Äù. Given such prohibitions likely require international coordination<span class="citation" data-cites="un2024"><sup><a href="#ref-un2024" role="doc-biblioref">35</a></sup></span>, Australia should have a continuing conversation about whether or not its representatives should support such a prohibition.</p>
<p>Overall, with <a href="#most-risks-are-seen-as-priorities">so many risks to prioritise</a>, and deep distrust of the technology<span class="citation" data-cites="gillespieTrustAttitudesUse2025"><sup><a href="#ref-gillespieTrustAttitudesUse2025" role="doc-biblioref">2</a></sup></span>, countries like Australia have a difficult job. Significant work is required to reduce risks to levels expected by the public. Still, our findings point to many democratically popular approaches for making AI both trusted and trustworthy.</p>
</section>
</section>
<section id="methodology" class="level1">
<h1>Methodology</h1>
<section id="survey-design-and-implementation" class="level2">
<h2 class="anchored" data-anchor-id="survey-design-and-implementation">Survey Design and Implementation</h2>
<section id="sample-recruitment" class="level3">
<h3 class="anchored" data-anchor-id="sample-recruitment">Sample Recruitment</h3>
<p>The survey was conducted with <strong>1069 Australian adults</strong> between <strong>August 14-30, 2025</strong>, recruited through Prolific to achieve demographic diversity. We used Prolific because comparison studies have suggested it provides some of the best data quality<span class="citation" data-cites="peer2022"><sup><a href="#ref-peer2022" role="doc-biblioref">36</a></sup></span>.</p>
</section>
<section id="quality-control" class="level3">
<h3 class="anchored" data-anchor-id="quality-control">Quality Control</h3>
<ul>
<li>Two attention check questions embedded in survey</li>
<li><strong>933 participants (87.3%)</strong> passed both checks</li>
<li>These 933 responses form our analysis sample</li>
<li>Validated Australian residential postcodes only</li>
<li>Median completion time analysis to identify ‚Äòspeeders‚Äô</li>
</ul>
</section>
<section id="randomisation" class="level3">
<h3 class="anchored" data-anchor-id="randomisation">Randomisation</h3>
<p>Key experimental manipulations:</p>
<ul>
<li>Module presentation order</li>
<li>Casualty numbers (10 to 8 billion)</li>
<li>Risk percentages (0.01% to 20%)</li>
<li>Delay periods (1 to 50 years)</li>
</ul>
</section>
</section>
<section id="statistical-approach" class="level2">
<h2 class="anchored" data-anchor-id="statistical-approach">Statistical Approach</h2>
<section id="multilevel-regression-and-post-stratification-mrp" class="level3">
<h3 class="anchored" data-anchor-id="multilevel-regression-and-post-stratification-mrp">Multilevel Regression and Post-stratification (MRP)</h3>
<p>MRP is the state-of-the-art method for generating population-representative estimates from non-probability samples<span class="citation" data-cites="downes2018 gelman2018"><sup><a href="#ref-downes2018" role="doc-biblioref">37</a>,<a href="#ref-gelman2018" role="doc-biblioref">38</a></sup></span>.</p>
<p><strong>How MRP Works:</strong></p>
<ol type="1">
<li><strong>Model individual responses</strong> using demographic and geographic predictors</li>
<li><strong>Generate predictions</strong> for all demographic-geographic combinations</li>
<li><strong>Weight predictions</strong> using Census population data</li>
<li><strong>Aggregate</strong> to population-level estimates with uncertainty</li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li>Corrects for any sampling bias</li>
<li>Provides uncertainty quantification</li>
<li>Validated in election, health, and public opinion contexts</li>
<li>More accurate than simple weighting</li>
</ul>
</section>
<section id="technical-specifications" class="level3">
<h3 class="anchored" data-anchor-id="technical-specifications">Technical Specifications</h3>
<ul>
<li><strong>Software</strong>: R with <code>brms</code> package</li>
<li><strong>Model</strong>: Bayesian multilevel regression</li>
<li><strong>Iterations</strong>: 2000 per chain, 1000 warmup</li>
<li><strong>Chains</strong>: 4 parallel chains</li>
<li><strong>Convergence</strong>: R-hat &lt; 1.01, ESS &gt; 400</li>
<li><strong>Post-stratification</strong>: 2021 Australian Census data</li>
</ul>
</section>
</section>
</section>
<section id="limitations" class="level1">
<h1>Limitations</h1>
<section id="methodological-considerations" class="level2">
<h2 class="anchored" data-anchor-id="methodological-considerations">Methodological Considerations</h2>
<p>While this study employs state-of-the-art methods, several limitations warrant careful consideration when interpreting results.</p>
<section id="selection-bias" class="level3">
<h3 class="anchored" data-anchor-id="selection-bias">Selection Bias</h3>
<p>Our online panel recruitment methodology likely oversamples individuals comfortable with technology:</p>
<ul>
<li><strong>Tech-savvy bias</strong>: Respondents willing to complete online surveys may be more familiar with AI than the general population</li>
<li><strong>Underestimated concerns</strong>: If tech-comfortable individuals express this level of concern (74% worry about under-regulation), actual population concerns may be even higher</li>
<li><strong>Limited reach</strong>: Digitally excluded populations‚Äîoften older, rural, or socioeconomically disadvantaged‚Äîare underrepresented despite MRP adjustments</li>
</ul>
<p>This selection bias suggests our findings may represent a <em>lower bound</em> on public concerns about AI risks.</p>
</section>
<section id="attention-check-validity" class="level3">
<h3 class="anchored" data-anchor-id="attention-check-validity">Attention Check Validity</h3>
<p>The 12.7% attention check failure rate raises important questions:</p>
<ul>
<li><strong>Different population segment?</strong> Attention check failures might represent a distinct group‚Äîperhaps those more sceptical of surveys or less engaged with technical topics</li>
<li><strong>Conservative analysis</strong>: By excluding these responses, we may have removed legitimate voices, particularly from less educated or engaged segments</li>
</ul>
<p>Future research should explore whether attention check ‚Äúfailures‚Äù represent a meaningful population subset with different AI attitudes.</p>
</section>
<section id="question-framing-effects" class="level3">
<h3 class="anchored" data-anchor-id="question-framing-effects">Question Framing Effects</h3>
<p>Several aspects of our question design may have influenced responses:</p>
<p><strong>Anchoring effects</strong>:</p>
<ul>
<li>Specifying ‚Äúglobal catastrophe (over 1 million deaths)‚Äù provides a concrete anchor that may shape risk perception</li>
<li>Comparing to known risks (aviation, nuclear) frames AI in terms of established dangerous technologies</li>
</ul>
<p><strong>Hypothetical scenarios</strong>:</p>
<ul>
<li>Asking about international treaties or 50-year delays involves speculation about unfamiliar concepts</li>
<li>Public may agree with precautionary measures without fully understanding implications</li>
</ul>
</section>
</section>
<section id="implications-of-limitations" class="level2">
<h2 class="anchored" data-anchor-id="implications-of-limitations">Implications of Limitations</h2>
<p>Despite these limitations, the findings provide valuable insights:</p>
<ol type="1">
<li><strong>Conservative estimates</strong>: Selection bias may mean we <em>underestimate</em> public concerns</li>
<li><strong>Robust patterns</strong>: The consistency of findings across measures suggests real underlying attitudes</li>
<li><strong>Policy relevance</strong>: Even if effects are partially driven by framing, they reflect how public will respond to real policy debates</li>
<li><strong>Action imperative</strong>: Limitations don‚Äôt negate the clear trust deficit and demand for governance</li>
</ol>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-productivitycommission2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Productivity Comission. <em>Harnessing Data and Digital Technology</em>. <a href="https://www.pc.gov.au/inquiries/current/data-digital/interim/data-digital-interim.pdf">https://www.pc.gov.au/inquiries/current/data-digital/interim/data-digital-interim.pdf</a> (2025).</div>
</div>
<div id="ref-gillespieTrustAttitudesUse2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Gillespie, N., Lockey, S. &amp; Curtis, C. <em>Trust, <span>Attitudes</span> and <span>Use</span> of <span>Artificial Intelligence</span>: <span>A Global Study</span> 2025</em>. (2025).</div>
</div>
<div id="ref-aiactionplan" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Office of Science and Technology Policy. <em>Winning the <span>Race</span>: <span>America</span>‚Äôs <span>AI Action Plan</span></em>. <a href="https://www.ai.gov/action-plan">https://www.ai.gov/action-plan</a> (2025).</div>
</div>
<div id="ref-oecd2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">OECD/BCG/INSEAD. <em>The <span>Adoption</span> of <span>Artificial Intelligence</span> in <span>Firms</span>: <span>New Evidence</span> for <span>Policymaking</span></em>. <a href="https://www.oecd.org/en/publications/the-adoption-of-artificial-intelligence-in-firms_f9ef33c3-en.html">https://www.oecd.org/en/publications/the-adoption-of-artificial-intelligence-in-firms_f9ef33c3-en.html</a> (2025) doi:<a href="https://doi.org/10.1787/f9ef33c3-en">10.1787/f9ef33c3-en</a>.</div>
</div>
<div id="ref-internationalassociationofprivacyprofessionals2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">International Association of Privacy Professionals. <a href="https://iapp.org/resources/article/consumer-perspectives-of-privacy-and-ai/">Consumer <span>Perspectives</span> of <span>Privacy</span> and <span>Artificial Intelligence</span></a>. (2024).</div>
</div>
<div id="ref-kingRethinkingPrivacyAI2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">King, T. &amp; Meinhardt, N. <em>Rethinking <span>Privacy</span> in the <span>AI Era</span>: <span>Policy Provocations</span> for a <span>Data-Centric World</span></em>. <a href="https://hai.stanford.edu/white-paper-rethinking-privacy-ai-era">https://hai.stanford.edu/white-paper-rethinking-privacy-ai-era</a> (2024).</div>
</div>
<div id="ref-europeanunionRegulationEU20242024" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">European Union. Regulation (<span>EU</span>) 2024/1689 <span>Laying</span> down <span>Harmonised Rules</span> on <span>Artificial Intelligence</span> (<span>Artificial Intelligence Act</span>). (2024).</div>
</div>
<div id="ref-ausvoluntaryguardrails" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Resources, D. of I. S. and. <a href="https://www.industry.gov.au/publications/voluntary-ai-safety-standard">Voluntary <span>AI Safety Standard</span> <span>Department</span> of <span>Industry Science</span> and <span>Resources</span></a>. <em>https://www.industry.gov.au/node/93303</em> (2025).</div>
</div>
<div id="ref-deltapoll2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Department for Science, Innovation and Technology and Centre for Data Ethics and Innovation. <a href="https://www.gov.uk/government/publications/international-survey-of-public-opinion-on-ai-safety">International survey of public opinion on <span>AI</span> safety</a>. <em>GOV.UK</em>.</div>
</div>
<div id="ref-floridiAI4PeopleAnEthicalFramework2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Floridi, L. <em>et al.</em> <a href="https://doi.org/10.1007/s11023-018-9482-5"><span>AI4People</span>‚Äî<span>An Ethical Framework</span> for a <span>Good AI Society</span>: <span>Opportunities</span>, <span>Risks</span>, <span>Principles</span>, and <span>Recommendations</span></a>. <em>Minds and Machines</em> <strong>28</strong>, 689‚Äì707 (2018).</div>
</div>
<div id="ref-winfieldEthicalGovernanceEssential2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Winfield, A. F. T. &amp; Jirotka, M. <a href="https://doi.org/10.1098/rsta.2018.0085">Ethical <span>Governance Is Essential</span> to <span>Building Trust</span> in <span>Robotics</span> and <span>Artificial Intelligence Systems</span></a>. <em>Philosophical Transactions of the Royal Society A</em> <strong>376</strong>, 20180085 (2018).</div>
</div>
<div id="ref-ipsos2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Ipsos. <em>Public <span>Attitudes</span> to <span>AI</span></em>. <a href="https://www.ipsos.com/sites/default/files/ct/publication/documents/2023-11/Public%20Perceptions%20of%20AI%20charts.pdf">https://www.ipsos.com/sites/default/files/ct/publication/documents/2023-11/Public%20Perceptions%20of%20AI%20charts.pdf</a> (2023).</div>
</div>
<div id="ref-ISRSAA2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Bengio, Y. <em>et al.</em> <em>International <span>AI Safety Report</span></em>. <a href="https://www.gov.uk/government/publications/international-ai-safety-report-2025">https://www.gov.uk/government/publications/international-ai-safety-report-2025</a> (2025).</div>
</div>
<div id="ref-saeri2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Saeri, A., Noetel, M. &amp; Graham, J. Survey <span>Assessing Risks</span> from <span>Artificial Intelligence</span> (<span>Technical Report</span>). (2024) doi:<a href="https://doi.org/10.2139/ssrn.4750953">10.2139/ssrn.4750953</a>.</div>
</div>
<div id="ref-lichtinger2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Lichtinger, G. &amp; Hosseini Maasoum, S. M. Generative <span>AI</span> as <span>Seniority-Biased Technological Change</span>: <span>Evidence</span> from <span>U</span>.<span>S</span>. <span>R√©sum√©</span> and <span>Job Posting Data</span>. (2025) doi:<a href="https://doi.org/10.2139/ssrn.5425555">10.2139/ssrn.5425555</a>.</div>
</div>
<div id="ref-pew2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">McClain, C., Kennedy, B., Gottfried, J., Anderson, M. &amp; Pasquini, G. <a href="https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/">How the <span>U</span>.<span>S</span>. <span>Public</span> and <span>AI Experts View Artificial Intelligence</span></a>. <em>Pew Research Center</em> (2025).</div>
</div>
<div id="ref-danielkokotajlo2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland &amp; Romeo Dean. <a href="https://ai-2027.com/"><span>AI</span> 2027</a>. <em>AI 2027</em> (2025).</div>
</div>
<div id="ref-ward2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Ward, T., Saeri, A. &amp; Noetel, M. Turn a <span>Blind AI</span>: <span>The Impact</span> of <span>Compassion Fade</span> and the <span>Identifiable Victim Effect</span> on <span>AI Risk Concerns</span>. (2024) doi:<a href="https://doi.org/10.2139/ssrn.4839839">10.2139/ssrn.4839839</a>.</div>
</div>
<div id="ref-disr2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Department of Industry, Science and Resources. <em>Safe and Responsible <span>AI</span> in <span>Australia</span>: <span>Proposals</span> Paper for Introducing Mandatory Guardrails for <span>AI</span> in High-Risk Settings</em>. <a href="https://storage.googleapis.com/converlens-au-industry/industry/p/prj2f6f02ebfe6a8190c7bdc/page/proposals_paper_for_introducing_mandatory_guardrails_for_ai_in_high_risk_settings.pdf">https://storage.googleapis.com/converlens-au-industry/industry/p/prj2f6f02ebfe6a8190c7bdc/page/proposals_paper_for_introducing_mandatory_guardrails_for_ai_in_high_risk_settings.pdf</a> (2024).</div>
</div>
<div id="ref-senatorwiener2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Senator Wiener. <a href="https://legiscan.com/CA/text/SB53/2025">Artificial intelligence models: Large developers</a>. (2025).</div>
</div>
<div id="ref-raise2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Micah Lasher, Rebecca Seawright, Amy Paulin &amp; Yudelka Tapia. <a href="https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A"><span>NY State Assembly Bill</span> 2025-<span>A6453A</span></a>. (2025).</div>
</div>
<div id="ref-seoul2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Department of Industry Science and Resources. <a href="https://www.industry.gov.au/publications/seoul-declaration-countries-attending-ai-seoul-summit-21-22-may-2024">The <span>Seoul Declaration</span> by countries attending the <span>AI Seoul Summit</span>, 21-22 <span>May</span> 2024</a>. <em>Department of Industry, Science and Resources</em> (2024).</div>
</div>
<div id="ref-aipicollab2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Artificial Intelligence Policy Institute. <a href="https://theaipi.org/voters-support-sb1047-in-collaborative-poll/">2.5x <span>More Voters Support SB1047</span> than <span>Oppose</span> it in <span>Collaborative Poll Between Anti-Bill Economist</span> and <span>AIPI</span> - <span>AI Policy Institute</span></a>. (2024).</div>
</div>
<div id="ref-aipisb1047" class="csl-entry" role="listitem">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Artificial Intelligence Policy Institute. <a href="https://theaipi.org/april-voters-prefer-ai-regulation-over-self-regulation-2-2/">Poll: <span>Californians Support Strong Version</span> of <span>SB1047</span>, <span>Disagree With Anthropic</span>‚Äôs <span>Proposed Changes</span> - <span>AI Policy Institute</span></a>. (2024).</div>
</div>
<div id="ref-iso31000" class="csl-entry" role="listitem">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">International Standards Organisation. <a href="https://www.iso.org/standard/65694.html"><span>ISO</span> 31000:2018</a>. (2018).</div>
</div>
<div id="ref-zhang2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Zhang, H. <em>et al.</em> Watermarks in the <span>Sand</span>: <span>Impossibility</span> of <span>Strong Watermarking</span> for <span>Generative Models</span>. (2025) doi:<a href="https://doi.org/10.48550/arXiv.2311.04378">10.48550/arXiv.2311.04378</a>.</div>
</div>
<div id="ref-ezrakarger2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Karger, E. <em>et al.</em> <em>Forecasting <span>Existential Risks</span>: <span>Evidence</span> from a <span>Long-Run Forecasting Tournament</span></em>. (2023).</div>
</div>
<div id="ref-graceThousandsAIAuthors2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">Grace, K. <em>et al.</em> <a href="https://arxiv.org/abs/2401.02843">Thousands of <span>AI Authors</span> on the <span>Future</span> of <span>AI</span></a>. <em>arXiv preprint arXiv:2401.02843</em> (2024).</div>
</div>
<div id="ref-bengioManagingAIRisks2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">Bengio, Y. <em>et al.</em> <a href="https://doi.org/10.1126/science.ado0724">Managing <span>AI Risks</span> in an <span>Era</span> of <span>Rapid Progress</span></a>. <em>Science</em> <strong>384</strong>, 842‚Äì845 (2024).</div>
</div>
<div id="ref-sloan2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Renieris, E. M., Kiron, D. &amp; Mills, S. <a href="https://sloanreview.mit.edu/article/a-fragmented-landscape-is-no-excuse-for-global-companies-serious-about-responsible-ai/">A <span>Fragmented Landscape Is No Excuse</span> for <span>Global Companies Serious About Responsible AI</span></a>. <em>MIT Sloan Management Review</em> (2024).</div>
</div>
<div id="ref-bengio2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Bengio, Y. <em>et al.</em> Superintelligent <span>Agents Pose Catastrophic Risks</span>: <span>Can Scientist AI Offer</span> a <span>Safer Path</span>? (2025) doi:<a href="https://doi.org/10.48550/arXiv.2502.15657">10.48550/arXiv.2502.15657</a>.</div>
</div>
<div id="ref-carnegiesb53" class="csl-entry" role="listitem">
<div class="csl-left-margin">32. </div><div class="csl-right-inline"><a href="https://carnegieendowment.org/emissary/2025/10/california-sb-53-frontier-ai-law-what-it-does?lang=en">California <span>Just Passed</span> the <span>First U</span>.<span>S</span>. <span>Frontier AI Law</span>. <span>Here</span>‚Äôs <span>What It Does</span>.</a> <em>Carnegie Endowment for International Peace</em>.</div>
</div>
<div id="ref-ussc2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">Shen, O. &amp; Lim, J. <a href="https://www.ussc.edu.au/landmark-laws-for-ai-transparency">Landmark laws for <span>AI</span> transparency</a>. <em>United States Studies Centre</em> (2025).</div>
</div>
<div id="ref-superstatement" class="csl-entry" role="listitem">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Future of Life Institute. <a href="https://superintelligence-statement.org">Statement on <span>Superintelligence</span></a>. <em>Statement on Superintelligence</em>.</div>
</div>
<div id="ref-un2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">United Nations. <em><a href="https://www.un.org/sites/un2.un.org/files/governing_ai_for_humanity_final_report_en.pdf">Govering <span>AI</span> for Humanity: Final Report</a></em>. (United Nations, New York, NY, 2024).</div>
</div>
<div id="ref-peer2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">Peer, E., Rothschild, D., Gordon, A., Evernden, Z. &amp; Damer, E. <a href="https://doi.org/10.3758/s13428-021-01694-3">Data quality of platforms and panels for online behavioral research</a>. <em>Behavior Research Methods</em> <strong>54</strong>, 1643‚Äì1662 (2022).</div>
</div>
<div id="ref-downes2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Downes, M. <em>et al.</em> <a href="https://doi.org/10.1093/aje/kwy070">Multilevel <span>Regression</span> and <span>Poststratification</span>: <span>A Modeling Approach</span> to <span>Estimating Population Quantities From Highly Selected Survey Samples</span></a>. <em>American Journal of Epidemiology</em> <strong>187</strong>, 1780‚Äì1790 (2018).</div>
</div>
<div id="ref-gelman2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">38. </div><div class="csl-right-inline">Gelman, A., Lax, J., Phillips, J., Gabry, J. &amp; Trangucci, R. <a href="https://sites.stat.columbia.edu/gelman/research/unpublished/MRT(1).pdf">Using <span>Multilevel Regression</span> and <span>Poststratification</span> to <span>Estimate Dynamic Public Opinion</span></a>. (2018).</div>
</div>
<div id="ref-iata2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">39. </div><div class="csl-right-inline">Said, H. <span>IATA Annual Safety Report</span> - 2024.</div>
</div>
<div id="ref-grace2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">40. </div><div class="csl-right-inline">Grace, K., Stein-Perlman, Z., Weinstein-Raun, B. &amp; Salvatier, J. <em>2022 <span>Expert Survey</span> on <span>Progress</span> in <span>AI</span></em>. <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a> (2022).</div>
</div>
<div id="ref-hipap4" class="csl-entry" role="listitem">
<div class="csl-left-margin">41. </div><div class="csl-right-inline">Department of Planning. <a href="https://www.planning.nsw.gov.au/sites/default/files/2023-03/hazardous-industry-planning-advisory-paper-no-4-risk-criteria-for-land-use-safety-planning.pdf"><span>HIPAP</span> 4: <span>Risk Criteria</span> for <span>Land Use Safety Planning</span></a>. (2011).</div>
</div>
<div id="ref-visualrisks2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">42. </div><div class="csl-right-inline">Lundgren, R. E. &amp; McMakin, A. H. Visual <span>Representations</span> of <span>Risks</span>. in <em>Risk <span>Communication</span></em> 159‚Äì191 (John Wiley &amp; Sons, Ltd, 2013). doi:<a href="https://doi.org/10.1002/9781118645734.ch14">10.1002/9781118645734.ch14</a>.</div>
</div>
<div id="ref-visschers2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">43. </div><div class="csl-right-inline">Visschers, V. H. M., Meertens, R. M., Passchier, W. W. F. &amp; De Vries, N. N. K. <a href="https://doi.org/10.1111/j.1539-6924.2008.01137.x">Probability <span>Information</span> in <span>Risk Communication</span>: <span>A Review</span> of the <span>Research Literature</span></a>. <em>Risk Analysis</em> <strong>29</strong>, 267‚Äì287 (2009).</div>
</div>
</div>
</section>
<section id="appendices" class="level1">
<h1>Appendices</h1>
<section id="appendix-a-survey-questions" class="level2">
<h2 class="anchored" data-anchor-id="appendix-a-survey-questions">Appendix A: Survey Questions</h2>
<section id="core-attitude-questions" class="level3">
<h3 class="anchored" data-anchor-id="core-attitude-questions">Core Attitude Questions</h3>
<p><strong>AI Good vs Harm</strong> ‚ÄúOverall, do you think artificial intelligence (AI) will do more good or more harm?‚Äù</p>
<ul>
<li>More good than harm</li>
<li>Neutral</li>
<li>More harm than good</li>
</ul>
<p><strong>Trust in Tech Companies</strong> ‚ÄúTo what extent do you trust tech companies to ensure the AI they develop is safe?‚Äù</p>
<ul>
<li>A great deal</li>
<li>A fair amount</li>
<li>Not very much</li>
<li>Not at all</li>
<li>Don‚Äôt know</li>
</ul>
<p><strong>Worry About Control</strong> ‚ÄúHow worried, if at all, are you that humans will lose control of AI?‚Äù</p>
<ul>
<li>Very worried</li>
<li>Fairly worried</li>
<li>Not very worried</li>
<li>Not at all worried</li>
<li>Don‚Äôt know</li>
</ul>
<p><strong>Worry About Job Loss</strong> ‚ÄúWhen it comes to artificial intelligence (AI), how worried are you about AI leading to job loss?‚Äù</p>
<ul>
<li>Very worried</li>
<li>Fairly worried</li>
<li>Not very worried</li>
<li>Not at all worried</li>
<li>Don‚Äôt know</li>
</ul>
<p><strong>International Treaty on Advanced AI</strong> ‚ÄúTo what extent would you support or oppose the introduction of an international treaty to ban any ‚Äòsmarter-than-human‚Äô artificial intelligence (AI)?‚Äù</p>
<ul>
<li>Strongly support</li>
<li>Somewhat support</li>
<li>Somewhat oppose</li>
<li>Strongly oppose</li>
<li>Don‚Äôt know</li>
</ul>
<p><strong>Regulation Pace</strong> ‚ÄúRegulation of AI in Australia is developing __________ than development in AI technologies.‚Äù</p>
<ul>
<li>Faster</li>
<li>Same Pace</li>
<li>Slower</li>
<li>Don‚Äôt know</li>
</ul>
<p><strong>Regulation Concern</strong> ‚ÄúThinking about the use of artificial intelligence (AI) in Australia, are you more concerned that the Australian government will go too far regulating its use or not go far enough regulating its use?‚Äù</p>
<ul>
<li>Go too far regulating its use</li>
<li>Not go far enough regulating its use</li>
<li>Not sure</li>
</ul>
<p><strong>Media Coverage of AI</strong> ‚ÄúI want to hear more from the media about how AI is going to affect society.‚Äù</p>
<ul>
<li>Strongly agree</li>
<li>Somewhat agree</li>
<li>Somewhat disagree</li>
<li>Strongly disagree</li>
<li>Not sure</li>
</ul>
<p><strong>Media Coverage of AI Regulation</strong> ‚ÄúI want to hear more from the media about how the government is regulating AI.‚Äù</p>
<ul>
<li>Strongly agree</li>
<li>Somewhat agree</li>
<li>Somewhat disagree</li>
<li>Strongly disagree</li>
<li>Not sure</li>
</ul>
<p><strong>Attention Check</strong> ‚ÄúThis one is not about AI. I have recently visited the moon.‚Äù</p>
<ul>
<li>Strongly agree</li>
<li>Somewhat agree</li>
<li>Somewhat disagree</li>
<li>Strongly disagree</li>
<li>Not sure</li>
</ul>
<p><strong>Barriers to AI Use</strong> ‚ÄúWhat are the main reasons you don‚Äôt use AI tools more often? (Select all that apply)‚Äù</p>
<ul>
<li>I don‚Äôt trust the companies that make them</li>
<li>I‚Äôm concerned about my data privacy</li>
<li>I think AI tools are unsafe or risky</li>
<li>I don‚Äôt understand how to use them</li>
<li>I‚Äôm worried about meeting my legal or professional obligations</li>
<li>I don‚Äôt like the culture around AI and tech companies</li>
<li>They‚Äôre not relevant to my work or interests</li>
<li>I prefer doing things without AI assistance</li>
<li>They‚Äôre too expensive</li>
<li>I‚Äôm concerned they might replace human jobs</li>
<li>Other</li>
</ul>
<p><strong>Government Focus</strong> ‚ÄúIf forced to prioritise, the Australian government should focus on:‚Äù</p>
<ul>
<li>Managing risks from AI</li>
<li>Driving innovation from AI</li>
</ul>
</section>
<section id="trust-building-measures" class="level3">
<h3 class="anchored" data-anchor-id="trust-building-measures">Trust-Building Measures</h3>
<p>For each measure, participants were asked: ‚ÄúI would be more likely to trust AI if‚Ä¶‚Äù</p>
<ul>
<li>there was an Australian AI Safety Institute that helps understand advanced AI, brief the Government on its risks, and work with industry on deploying it safely</li>
<li>there was an Australian AI regulator who sets and enforces laws and safety standards</li>
<li>technology companies were required to submit their most powerful AI models to a regulator for safety testing before being deployed</li>
<li>developers of frontier AI models were required to create and publish detailed safety and security protocols before building the system</li>
<li>large AI developers were required to undergo annual independent audits of their safety compliance</li>
<li>AI developers were required to report safety incidents (like model escapes, unauthorised access, or autonomous behaviour) to authorities within 72 hours</li>
<li>employees were protected from retaliation when reporting AI safety concerns, even if their employer is technically following the law</li>
<li>there were rules preventing AI developers from making contracts that remove their liability for harms</li>
<li>developers of large AI models were liable for catastrophic harms caused by their models</li>
<li>AI models had emergency shutdown capabilities such that model developers can disable their model in case of an emergency</li>
<li>organisations were required to conduct risk assessments before deploying AI systems that could impact human rights, health and safety, or have significant societal effects</li>
<li>organisations were required to inform people when AI was being used to make decisions about them (such as in hiring, lending, or government services)</li>
<li>AI-generated content (text, images, audio, video) was required to be labeled or watermarked so people knew it was created by AI</li>
<li>people had the right to challenge decisions made by AI systems and have their complaints reviewed by a human</li>
</ul>
</section>
<section id="risk-priorities" class="level3">
<h3 class="anchored" data-anchor-id="risk-priorities">Risk Priorities</h3>
<p>‚ÄúIn managing risks from AI, I think the government should focus on‚Ä¶‚Äù</p>
<ul>
<li>stopping bad actors from using AI to create fake videos or images that hurt people (like scams or fake nude photos)</li>
<li>stopping people from using AI-made fake content to trick the public</li>
<li>stopping hackers from using AI to attack computers, businesses, and important services like power grids</li>
<li>stopping AI from making it easier to create dangerous weapons (like biological or chemical weapons)</li>
<li>making sure AI systems work properly and don‚Äôt make mistakes or give false information</li>
<li>stopping AI systems from being unfair or treating some groups of people worse than others</li>
<li>preventing situations where AI systems could get out of human control</li>
<li>dealing with how AI affects people‚Äôs jobs and employment</li>
<li>making sure all countries have fair access to AI technology (not just rich countries)</li>
<li>preventing problems that happen when only a few big companies control all the AI technology</li>
<li>reducing the environmental damage from AI‚Äôs energy use</li>
<li>protecting people‚Äôs privacy from AI that collects personal data or watches what people do</li>
<li>dealing with problems when AI uses other people‚Äôs work without permission</li>
<li>I just want experts to focus on preventing whatever has the biggest risk</li>
<li>I just want experts to focus on helping the economy adapt to the change</li>
</ul>
</section>
<section id="risk-tolerance-questions" class="level3">
<h3 class="anchored" data-anchor-id="risk-tolerance-questions">Risk Tolerance Questions</h3>
<p><strong>Casualty Risk</strong> (with randomised casualty number N) ‚ÄúSome experts think that advanced AI could potentially cause accidents where people die. What is the HIGHEST probability you would tolerate of AI causing [N] casualties each year?‚Äù</p>
<ul>
<li>1% or higher (10,000+ in 100,000 chance) Similar to risk of a global pandemic</li>
<li>0.1% (100 in 100,000 chance) Similar to risk of death tolerated for workplace accidents</li>
<li>0.01% (10 in 100,000 chance) Similar to risk of death tolerated from dam failure</li>
<li>0.001% (1 in 100,000 chance) Similar to risk of death tolerated from nuclear power</li>
<li>0.0001% (0.1 in 100,000 chance) Similar to risk of death tolerated from commercial planes</li>
<li>0.00001% or less (‚â§0.01 in 100,000 chance) Similar to risk of extinction-level asteroid</li>
<li>Zero risk only is acceptable</li>
<li>This is too speculative to answer meaningfully</li>
</ul>
<p><strong>Risk-Benefit Trade-off</strong> (with randomised risk percentage X) ‚ÄúAn advanced AI system has a [X]% chance of causing a global catastrophe (over 1 million deaths) but a 60% chance of solving climate change and extending human lifespan by 20 years. This trade-off is:‚Äù</p>
<ul>
<li>Completely unacceptable</li>
<li>Probably unacceptable</li>
<li>Probably acceptable</li>
<li>Clearly acceptable</li>
<li>Cannot say</li>
</ul>
<p><strong>AI versus Existing Technologies</strong> ‚ÄúDifferent technologies have different safety standards. Compared to commercial airlines (less than 1 in 10,000,000 risk of death per flight), advanced AI systems should be:‚Äù</p>
<ul>
<li>Much stricter safety standards</li>
<li>Somewhat stricter standards</li>
<li>Same safety standards</li>
<li>Somewhat less strict</li>
<li>Much less strict</li>
<li>Cannot compare these technologies</li>
</ul>
<p><strong>Safety Standards Attention Check</strong> ‚ÄúVarious technologies require safety testing. To ensure you‚Äôre reading carefully, please select ‚ÄòSomewhat stricter than nuclear power‚Äô for this question about safety standards:‚Äù</p>
<ul>
<li>Much stricter than nuclear power</li>
<li>Somewhat stricter than nuclear power</li>
<li>Same as nuclear power</li>
<li>Somewhat less strict than nuclear power</li>
<li>Much less strict than nuclear power</li>
<li>No standards needed</li>
</ul>
<p><strong>Willingness to Pay for Safety</strong> ‚ÄúWhat is the maximum you would personally pay annually (through taxes/prices) to reduce AI catastrophic risk (over 1 million deaths) from 1% to 0.01% over the next 30 years?‚Äù</p>
<ul>
<li>$0 - Not worth paying for</li>
<li>$1-25</li>
<li>$26-100</li>
<li>$101-500</li>
<li>$501-1000</li>
<li>More than $1000</li>
<li>Cannot put price on this</li>
</ul>
<p><strong>Temporal Trade-off</strong> ‚ÄúIf delaying advanced AI deployment by [Delay] years would reduce catastrophic risk from 5% to 0.5%, this delay would be:‚Äù</p>
<ul>
<li>Clearly worthwhile</li>
<li>Probably worthwhile</li>
<li>Difficult to judge</li>
<li>Probably not worthwhile</li>
<li>Clearly not worthwhile</li>
</ul>
<p><strong>Consistency Validation</strong> ‚ÄúEarlier you indicated [Threshold] was your maximum acceptable risk for [Casualty] deaths from AI. To confirm, a technology with a [Threshold] chance of causing [Casualty] deaths is:‚Äù</p>
<ul>
<li>Far too risky</li>
<li>Somewhat too risky</li>
<li>About right</li>
<li>Could accept somewhat higher risk</li>
<li>Could accept much higher risk</li>
</ul>
<hr>
</section>
</section>
<section id="appendix-b-full-label-plots" class="level2">
<h2 class="anchored" data-anchor-id="appendix-b-full-label-plots">Appendix B: Full-Label Plots</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-trust-full-appendix" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trust-full-appendix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/FINAL_trust_measures_full.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trust-full-appendix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Trust measures with complete question wording
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-priority-full-appendix" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-priority-full-appendix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/FINAL_priority_risks_full.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-priority-full-appendix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Priority risks with complete question wording
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="appendix-c-statistical-details" class="level2">
<h2 class="anchored" data-anchor-id="appendix-c-statistical-details">Appendix C: Statistical Details</h2>
<section id="model-specifications" class="level3">
<h3 class="anchored" data-anchor-id="model-specifications">Model Specifications</h3>
<p><strong>Binary Outcomes Model:</strong></p>
<pre><code>y ~ 1 + (1 | state) + (1 | age_group) + (1 | education) + (1 | gender)</code></pre>
<p><strong>Ordinal Outcomes Model:</strong></p>
<pre><code>y ~ 1 + (1 | state) + (1 | age_group) + (1 | education) + (1 | gender)
family = cumulative(link = "logit")</code></pre>
</section>
<section id="convergence-diagnostics" class="level3">
<h3 class="anchored" data-anchor-id="convergence-diagnostics">Convergence Diagnostics</h3>
<p>All models achieved:</p>
<ul>
<li>R-hat values &lt; 1.01 for all parameters</li>
<li>Effective sample size &gt; 400 for all parameters</li>
<li>No divergent transitions</li>
<li>Visual inspection of trace plots showed good mixing</li>
</ul>
<hr>
</section>
</section>
<section id="appendix-d-sample-demographics" class="level2">
<h2 class="anchored" data-anchor-id="appendix-d-sample-demographics">Appendix D: Sample Demographics</h2>
<section id="sample-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="sample-characteristics">Sample Characteristics</h3>
<div class="cell" data-layout-align="center" data-tbl-cap="Sample demographics compared to 2021 Australian Census">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Sample demographics compared to 2021 Australian Census</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Characteristic</th>
<th style="text-align: right;">Sample (n=932)</th>
<th style="text-align: right;">Census 2021</th>
<th style="text-align: right;">Difference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Age</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">18-34</td>
<td style="text-align: right;">49.1%</td>
<td style="text-align: right;">32.0%</td>
<td style="text-align: right;">+17.1pp</td>
</tr>
<tr class="odd">
<td style="text-align: left;">35-54</td>
<td style="text-align: right;">39.3%</td>
<td style="text-align: right;">35.0%</td>
<td style="text-align: right;">+4.3pp</td>
</tr>
<tr class="even">
<td style="text-align: left;">55+</td>
<td style="text-align: right;">11.6%</td>
<td style="text-align: right;">33.0%</td>
<td style="text-align: right;">-21.4pp</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gender</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Woman</td>
<td style="text-align: right;">49.6%</td>
<td style="text-align: right;">50.0%</td>
<td style="text-align: right;">-0.4pp</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Man</td>
<td style="text-align: right;">49.5%</td>
<td style="text-align: right;">50.0%</td>
<td style="text-align: right;">-0.5pp</td>
</tr>
<tr class="even">
<td style="text-align: left;">Other/Prefer not to say</td>
<td style="text-align: right;">1.0%</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Education</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">No university degree</td>
<td style="text-align: right;">32.1%</td>
<td style="text-align: right;">65.0%</td>
<td style="text-align: right;">-32.9pp</td>
</tr>
<tr class="odd">
<td style="text-align: left;">University degree</td>
<td style="text-align: right;">67.9%</td>
<td style="text-align: right;">35.0%</td>
<td style="text-align: right;">+32.9pp</td>
</tr>
<tr class="even">
<td style="text-align: left;">State</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">VIC</td>
<td style="text-align: right;">31.5%</td>
<td style="text-align: right;">26.0%</td>
<td style="text-align: right;">+5.5pp</td>
</tr>
<tr class="even">
<td style="text-align: left;">NSW</td>
<td style="text-align: right;">28.0%</td>
<td style="text-align: right;">32.0%</td>
<td style="text-align: right;">-4.0pp</td>
</tr>
<tr class="odd">
<td style="text-align: left;">QLD</td>
<td style="text-align: right;">19.0%</td>
<td style="text-align: right;">20.0%</td>
<td style="text-align: right;">-1.0pp</td>
</tr>
<tr class="even">
<td style="text-align: left;">SA</td>
<td style="text-align: right;">8.5%</td>
<td style="text-align: right;">7.0%</td>
<td style="text-align: right;">+1.5pp</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WA</td>
<td style="text-align: right;">7.2%</td>
<td style="text-align: right;">10.0%</td>
<td style="text-align: right;">-2.8pp</td>
</tr>
<tr class="even">
<td style="text-align: left;">TAS/ACT/NT</td>
<td style="text-align: right;">5.8%</td>
<td style="text-align: right;">5.0%</td>
<td style="text-align: right;">+0.8pp</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The sample shows reasonable demographic representativeness with slight oversampling of younger, university-educated respondents, typical for online panels. Multi-level regression and post-stratification methods are robust to small deviations from census proportions like these.</p>
<hr>
</section>
</section>
<section id="appendix-e-demographic-heterogeneity-analysis" class="level2">
<h2 class="anchored" data-anchor-id="appendix-e-demographic-heterogeneity-analysis">Appendix E: Demographic Heterogeneity Analysis</h2>
<section id="key-attitudes-by-age-group" class="level3">
<h3 class="anchored" data-anchor-id="key-attitudes-by-age-group">Key Attitudes by Age Group</h3>
<div class="cell" data-layout-align="center" data-tbl-cap="Support for key measures by age group with 95% confidence intervals">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Support for key measures by age group with 95% confidence intervals</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Question</th>
<th style="text-align: right;">Sample Size_18-34</th>
<th style="text-align: right;">Sample Size_35-54</th>
<th style="text-align: right;">Sample Size_55+</th>
<th style="text-align: left;">Estimate [95% CI]_18-34</th>
<th style="text-align: left;">Estimate [95% CI]_35-54</th>
<th style="text-align: left;">Estimate [95% CI]_55+</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Trust tech companies (% trust)</td>
<td style="text-align: right;">451</td>
<td style="text-align: right;">353</td>
<td style="text-align: right;">107</td>
<td style="text-align: left;">27.7% [23.6-31.8]</td>
<td style="text-align: left;">20.4% [16.2-24.6]</td>
<td style="text-align: left;">19.6% [12.1-27.2]</td>
</tr>
<tr class="even">
<td style="text-align: left;">Worried about loss of control (% worried)</td>
<td style="text-align: right;">442</td>
<td style="text-align: right;">357</td>
<td style="text-align: right;">106</td>
<td style="text-align: left;">50.9% [46.2-55.6]</td>
<td style="text-align: left;">56.3% [51.2-61.4]</td>
<td style="text-align: left;">59.4% [50.1-68.8]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Worried about job loss (% worried)</td>
<td style="text-align: right;">449</td>
<td style="text-align: right;">363</td>
<td style="text-align: right;">107</td>
<td style="text-align: left;">69.5% [65.2-73.7]</td>
<td style="text-align: left;">57.3% [52.2-62.4]</td>
<td style="text-align: left;">51.4% [41.9-60.9]</td>
</tr>
<tr class="even">
<td style="text-align: left;">Support AGI treaty ban (% support)</td>
<td style="text-align: right;">381</td>
<td style="text-align: right;">283</td>
<td style="text-align: right;">93</td>
<td style="text-align: left;">56.2% [51.2-61.2]</td>
<td style="text-align: left;">54.1% [48.3-59.9]</td>
<td style="text-align: left;">58.1% [48.0-68.1]</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="key-attitudes-by-education-level" class="level3">
<h3 class="anchored" data-anchor-id="key-attitudes-by-education-level">Key Attitudes by Education Level</h3>
<div class="cell" data-layout-align="center" data-tbl-cap="Support for key measures by education level">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Support for key measures by education level</caption>
<colgroup>
<col style="width: 29%">
<col style="width: 16%">
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Question</th>
<th style="text-align: right;">n_No university degree</th>
<th style="text-align: right;">n_University degree</th>
<th style="text-align: left;">estimate_No university degree</th>
<th style="text-align: left;">estimate_University degree</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Trust tech companies (% trust)</td>
<td style="text-align: right;">289</td>
<td style="text-align: right;">622</td>
<td style="text-align: left;">22.5% [17.7-27.3]</td>
<td style="text-align: left;">24.6% [21.2-28.0]</td>
</tr>
<tr class="even">
<td style="text-align: left;">Worried about loss of control (% worried)</td>
<td style="text-align: right;">289</td>
<td style="text-align: right;">616</td>
<td style="text-align: left;">58.5% [52.8-64.2]</td>
<td style="text-align: left;">51.9% [48.0-55.9]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Worried about job loss (% worried)</td>
<td style="text-align: right;">292</td>
<td style="text-align: right;">627</td>
<td style="text-align: left;">62.7% [57.1-68.2]</td>
<td style="text-align: left;">62.5% [58.7-66.3]</td>
</tr>
<tr class="even">
<td style="text-align: left;">Support AGI treaty ban (% support)</td>
<td style="text-align: right;">244</td>
<td style="text-align: right;">513</td>
<td style="text-align: left;">57.0% [50.8-63.2]</td>
<td style="text-align: left;">55.0% [50.7-59.3]</td>
</tr>
</tbody>
</table>
</div>
</div>
<hr>
</section>
</section>
<section id="appendix-f-robustness-checks" class="level2">
<h2 class="anchored" data-anchor-id="appendix-f-robustness-checks">Appendix F: Robustness Checks</h2>
<section id="attention-check-performance" class="level3">
<h3 class="anchored" data-anchor-id="attention-check-performance">Attention Check Performance</h3>
<p>All main analyses use only participants who passed both attention checks (n=933).This table shows the distribution of attention check performance in the full sample.</p>
<div class="cell" data-layout-align="center" data-tbl-cap="Response differences by attention check performance">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Response differences by attention check performance</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Attention Check Performance</th>
<th style="text-align: right;">N</th>
<th style="text-align: left;">% of Total Sample</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Failed both</td>
<td style="text-align: right;">26</td>
<td style="text-align: left;">2.4%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Passed both</td>
<td style="text-align: right;">933</td>
<td style="text-align: left;">87.3%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Passed one</td>
<td style="text-align: right;">110</td>
<td style="text-align: left;">10.3%</td>
</tr>
</tbody>
</table>
</div>
</div>
<p><strong>Robustness Finding:</strong> Excluding attention check failures does not materially change results. Main estimates shift by &lt;3 percentage points.</p>
<hr>
</section>
</section>
<section id="sec-appendix-g" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-g">Appendix G: Comparison Between Risk Tolerance and Forecast Risk Levels</h2>
<section id="setting-the-baseline-aviation-safety-standards" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-baseline-aviation-safety-standards">Setting the Baseline: Aviation Safety Standards</h3>
<p>Commercial aviation achieves:</p>
<ul>
<li><strong>144 deaths globally per year</strong> from commercial flights (IATA 2024 rolling average<span class="citation" data-cites="iata2024"><sup><a href="#ref-iata2024" role="doc-biblioref">39</a></sup></span>)</li>
<li>Global population: ~8 billion</li>
<li><strong>Individual annual risk: 1.8 √ó 10‚Åª‚Å∏ per person per year</strong> (144/8,000,000,000)</li>
</ul>
<p>This is the safety standard the public expects for AI.</p>
</section>
<section id="converting-xpt-forecasts-to-annual-per-person-risk" class="level3">
<h3 class="anchored" data-anchor-id="converting-xpt-forecasts-to-annual-per-person-risk">Converting XPT Forecasts to Annual Per-Person Risk</h3>
<p>From the XPT data<span class="citation" data-cites="ezrakarger2023"><sup><a href="#ref-ezrakarger2023" role="doc-biblioref">27</a></sup></span>, we have cumulative risks by 2100 (approximately 75 years from now):</p>
<section id="catastrophic-risk-10-of-humanity-dies" class="level4">
<h4 class="anchored" data-anchor-id="catastrophic-risk-10-of-humanity-dies">Catastrophic Risk (&gt;10% of humanity dies)</h4>
<ul>
<li><strong>Domain experts</strong>: 12% cumulative risk by 2100</li>
<li><strong>Superforecasters</strong>: 2.13% cumulative risk by 2100</li>
</ul>
</section>
<section id="extinction-risk-humanity-extinct-or-5000-survivors" class="level4">
<h4 class="anchored" data-anchor-id="extinction-risk-humanity-extinct-or-5000-survivors">Extinction Risk (humanity extinct or &lt;5000 survivors)</h4>
<ul>
<li><strong>Domain experts</strong>: 3% cumulative risk by 2100</li>
<li><strong>Superforecasters</strong>: 0.38% cumulative risk by 2100</li>
</ul>
</section>
<section id="step-1-annualizing-cumulative-risk" class="level4">
<h4 class="anchored" data-anchor-id="step-1-annualizing-cumulative-risk">Step 1: Annualizing Cumulative Risk</h4>
<p>For a cumulative risk P over n years, assume the constant annual risk r is:</p>
<p>1 - (1 - r)‚Åø = P therefore: r = 1 - (1 - P)^(1/n)</p>
</section>
<section id="step-2-catastrophic-risk-calculations" class="level4">
<h4 class="anchored" data-anchor-id="step-2-catastrophic-risk-calculations">Step 2: Catastrophic Risk Calculations</h4>
<p><strong>Domain Experts:</strong></p>
<ul>
<li>r = 1 - (1 - 0.12)^(1/75) = 1 - 0.88^(1/75) ‚âà <strong>0.00170</strong> annual probability</li>
<li>If catastrophe occurs, conservatively assuming minimum 10% die</li>
<li>Individual risk: 0.00170 √ó 0.1 = <strong>1.70 √ó 10‚Åª‚Å¥</strong> per person per year</li>
</ul>
<p><strong>Superforecasters:</strong></p>
<ul>
<li>r = 1 - (1 - 0.0213)^(1/75) = 1 - 0.9787^(1/75) ‚âà <strong>0.000287</strong> annual probability</li>
<li>Individual risk: 0.000287 √ó 0.1 = <strong>2.87 √ó 10‚Åª‚Åµ</strong> per person per year</li>
</ul>
</section>
<section id="step-3-extinction-risk-calculations" class="level4">
<h4 class="anchored" data-anchor-id="step-3-extinction-risk-calculations">Step 3: Extinction Risk Calculations</h4>
<p><strong>Domain Experts:</strong></p>
<ul>
<li>r = 1 - (1 - 0.03)^(1/75) = 1 - 0.97^(1/75) ‚âà <strong>0.000408</strong> annual probability</li>
<li>Individual risk (100% mortality): <strong>4.08 √ó 10‚Åª‚Å¥</strong> per person per year</li>
</ul>
<p><strong>Superforecasters:</strong></p>
<ul>
<li>r = 1 - (1 - 0.0038)^(1/75) = 1 - 0.9962^(1/75) ‚âà <strong>0.0000508</strong> annual probability</li>
<li>Individual risk: <strong>5.08 √ó 10‚Åª‚Åµ</strong> per person per year</li>
</ul>
</section>
<section id="step-4-combined-risk-assessment" class="level4">
<h4 class="anchored" data-anchor-id="step-4-combined-risk-assessment">Step 4: Combined Risk Assessment</h4>
<p>Extinction events are a subset of catastrophic scenarios. To avoid double counting while still using conservative assumptions (10% fatalities for non-extinction catastrophes, 100% for extinction):</p>
<p><strong>Domain Experts Minimum Annual Per-Person Death Risk:</strong></p>
<ul>
<li>Non-extinction catastrophic component: (0.00170 ‚àí 0.000408) √ó 0.1 ‚âà 1.29 √ó 10‚Åª‚Å¥</li>
<li>Extinction component: 4.08 √ó 10‚Åª‚Å¥</li>
<li><strong>Combined minimum: ~5.37 √ó 10‚Åª‚Å¥</strong></li>
</ul>
<p><strong>Superforecasters Minimum Annual Per-Person Death Risk:</strong></p>
<ul>
<li>Non-extinction catastrophic component: (0.000287 ‚àí 0.0000508) √ó 0.1 ‚âà 2.36 √ó 10‚Åª‚Åµ</li>
<li>Extinction component: 5.08 √ó 10‚Åª‚Åµ</li>
<li><strong>Combined minimum: ~7.44 √ó 10‚Åª‚Åµ</strong></li>
</ul>
</section>
</section>
<section id="key-comparison" class="level3">
<h3 class="anchored" data-anchor-id="key-comparison">Key Comparison</h3>
<p>Comparing to aviation safety standard of <strong>1.8 √ó 10‚Åª‚Å∏ per person per year</strong>:</p>
<section id="primary-comparison" class="level4">
<h4 class="anchored" data-anchor-id="primary-comparison">Primary Comparison</h4>
<ul>
<li><strong>Domain Expert Assessment</strong>: 5.37 √ó 10‚Åª‚Å¥ √∑ 1.8 √ó 10‚Åª‚Å∏ = <strong>~29,833 times higher</strong> than aviation safety</li>
<li><strong>Superforecaster Assessment</strong>: 7.44 √ó 10‚Åª‚Åµ √∑ 1.8 √ó 10‚Åª‚Å∏ = <strong>~4,133 times higher</strong> than aviation safety</li>
</ul>
</section>
</section>
<section id="contextualizing-the-gap" class="level3">
<h3 class="anchored" data-anchor-id="contextualizing-the-gap">Contextualizing the Gap</h3>
<p>To put this in perspective:</p>
<ul>
<li><strong>Commercial Aviation</strong>: Kills ~144 people per year globally (IATA 2024 rolling average)</li>
<li><strong>AI (Domain Expert Assessment)</strong>: Expected damage value (EDV = fatalities x probability) of ~4.3 million people per year on average (5.37 √ó 10‚Åª‚Å¥ √ó 8 billion)</li>
<li><strong>AI (Superforecaster Assessment)</strong>: Expected damage value (EDV) of ~595,000 people per year on average (7.44 √ó 10‚Åª‚Åµ √ó 8 billion)</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p><em>The current expert-assessed AI risk is</em> <strong><em>4,000 to 30,000 times higher</em></strong> <em>than aviation safety standards that the public expects.</em></p>
</section>
<section id="why-the-xpt-numbers-are-trustworthy-tournament-design-and-participant-credibility" class="level3">
<h3 class="anchored" data-anchor-id="why-the-xpt-numbers-are-trustworthy-tournament-design-and-participant-credibility">Why the XPT Numbers Are Trustworthy: Tournament Design and Participant Credibility</h3>
<section id="the-tournament-structure" class="level4">
<h4 class="anchored" data-anchor-id="the-tournament-structure">The Tournament Structure</h4>
<p>The <strong>Existential Risk Persuasion Tournament (XPT)</strong><span class="citation" data-cites="ezrakarger2023"><sup><a href="#ref-ezrakarger2023" role="doc-biblioref">27</a></sup></span> was a rigorous, multi-stage forecasting tournament conducted in 2022-2023 with 169 participants who spent months developing and refining their risk assessments. The tournament was explicitly designed to produce the highest-quality possible estimates of existential risks.</p>
</section>
<section id="key-credibility-factors" class="level4">
<h4 class="anchored" data-anchor-id="key-credibility-factors">Key Credibility Factors</h4>
<ol type="1">
<li><strong>Superforecasters: Proven Track Record</strong></li>
</ol>
<p>The tournament included <strong>88 superforecasters</strong>: individuals with demonstrated exceptional accuracy in predicting geopolitical and economic events. These are people who:</p>
<ul>
<li>Have been empirically validated through years of forecasting tournaments</li>
<li>Beat prediction markets and expert panels on near-term questions</li>
<li>Were recruited from the top performers in previous tournaments run by Good Judgment Inc.</li>
</ul>
<ol start="2" type="1">
<li><strong>Domain Experts: Deep Technical Knowledge</strong></li>
</ol>
<p>The tournament included <strong>30 AI domain experts</strong>, including:</p>
<ul>
<li>Employees at major AI companies (names withheld for confidentiality)</li>
<li>Academic researchers specialising in AI safety and capabilities</li>
</ul>
<ol start="3" type="1">
<li><strong>Incentive Structure for Accuracy</strong></li>
</ol>
<p>Participants were:</p>
<ul>
<li><strong>Financially incentivized</strong> for accurate forecasts (prizes for best performers)</li>
<li><strong>Scored on calibration</strong> - not just point estimates but also confidence intervals</li>
<li><strong>Rewarded for persuasion</strong> - bonuses for convincing others, incentivizing sharing of best arguments</li>
</ul>
<ol start="4" type="1">
<li><strong>Extensive Deliberation Process</strong></li>
</ol>
<p>The tournament involved:</p>
<ul>
<li><strong>Multiple stages</strong> over several months</li>
<li><strong>Millions of words</strong> of written rationales and arguments</li>
<li><strong>Thousands of forecasts</strong> with detailed justifications</li>
<li>Team formation where forecasters could <strong>debate and challenge each other</strong></li>
</ul>
<p>This iterative process typically leads to more accurate assessments than one-off surveys.</p>
<ol start="5" type="1">
<li><strong>Transparency in Reasoning</strong></li>
</ol>
<p>Unlike black-box predictions, the XPT required participants to:</p>
<ul>
<li>Provide detailed written rationales for their forecasts</li>
<li>Explicitly state their assumptions</li>
<li>Respond to counterarguments</li>
<li>Show their reasoning chains</li>
</ul>
<p><em>This transparency allows us to evaluate the quality of reasoning, not just trust the numbers.</em></p>
</section>
<section id="calibration-against-prior-expert-surveys" class="level4">
<h4 class="anchored" data-anchor-id="calibration-against-prior-expert-surveys">Calibration Against Prior Expert Surveys</h4>
<p>The XPT results align with other expert assessments, such as the 2022<span class="citation" data-cites="grace2022"><sup><a href="#ref-grace2022" role="doc-biblioref">40</a></sup></span> and 2024<span class="citation" data-cites="graceThousandsAIAuthors2024"><sup><a href="#ref-graceThousandsAIAuthors2024" role="doc-biblioref">28</a></sup></span> researcher survey estimating a 5% median chance of AI causing human extinction.</p>
</section>
<section id="methodological-strengths" class="level4">
<h4 class="anchored" data-anchor-id="methodological-strengths">Methodological Strengths</h4>
<ol type="1">
<li><strong>Proper scoring rules</strong> that mathematically incentivize honest probability assessments</li>
<li><strong>Separation of different risk levels</strong> (catastrophic vs.&nbsp;extinction)</li>
<li><strong>Multiple time horizons</strong> (2030, 2050, 2100) allowing for consistency checks</li>
<li><strong>Cross-domain comparison</strong> (AI risks assessed alongside nuclear, bio, and other risks)</li>
</ol>
</section>
<section id="conservative-aspects-of-these-estimates" class="level4">
<h4 class="anchored" data-anchor-id="conservative-aspects-of-these-estimates">Conservative Aspects of These Estimates</h4>
<p>Several factors suggest these estimates might actually be <em>conservative</em>:</p>
<ol type="1">
<li><strong>Selection bias toward sceptics</strong>: Superforecasters are selected for accuracy on near-term, conventional events - not tail risks</li>
<li><strong>Pre-GPT-4 era</strong>: The tournament concluded before recent AI capabilities advances</li>
</ol>
<hr>
</section>
</section>
</section>
<section id="sec-appendix-h" class="level2">
<h2 class="anchored" data-anchor-id="sec-appendix-h">Appendix H: Testing Robustness of Risk Tolerance and Scope Sensitivity</h2>
<p>One goal of this survey was to quantify the public‚Äôs risk tolerances around AI. As noted above, risk tolerances help regulators decide what precautions are required to meet the public‚Äôs expectations<span class="citation" data-cites="hipap4"><sup><a href="#ref-hipap4" role="doc-biblioref">41</a></sup></span>. These tolerances are often presented as Frequency-Consequence graphs, because the public naturally expect a lower risk of bigger threats. For example, the following are the criteria used by the New South Wales government for land use<span class="citation" data-cites="hipap4"><sup><a href="#ref-hipap4" role="doc-biblioref">41</a></sup></span>.</p>
<p>It says that an ‚Äòintolerable‚Äô risk of 1 death should be 10^-3 or 0.001 or 1 in 1,000. Safety planning tries to keep the risk below the intolerable levels. It says no risk of more than 1,000 deaths is ‚Äòtolerable‚Äô. A ‚Äònegligible‚Äô risk of 1 death is 10^-5 or 0.00001 or 1 in 100,000. As this level of safety is usually expensive, so risk management usually aims to keep risks in the ALARP region (as low as reasonably practical).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/societal_risk_criteria.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>New South Wales Government‚Äôs Societal Risk Criteria for Land Use Safety Planning</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>We aimed to explore whether such curves could be drafted for AI.</strong></p>
<p><strong>Question Design:</strong> Each respondent was randomly shown a potential casualty number (ranging from 10 people to 8 billion people) and asked: ‚ÄúWhat is the HIGHEST probability you would tolerate of AI causing (x) casualties each year?‚Äù Participants responded via a risk ladder. Risk ladders increase understanding of risks by presenting probabilities in multiple formats, presented vertically in a ladder<span class="citation" data-cites="visualrisks2013"><sup><a href="#ref-visualrisks2013" role="doc-biblioref">42</a></sup></span>. Our risk ladder drew on recommendations around risk communication<span class="citation" data-cites="visschers2009"><sup><a href="#ref-visschers2009" role="doc-biblioref">43</a></sup></span> by presenting risks as both percentages and natural frequencies with consistent denominators (x in 100,000), anchored to comparable risks (societal risks from dam failure, nuclear power, commercial aviation).</p>
<p><a href="#australians-expect-ai-to-be-as-safe-as-commercial-aviation">As above</a>, the public expect AI to have the same failure rate as commercial aviation (1 in 1,000,000 chance of death per year). As seen below, Australians were scope insensitive<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, selecting very similar risk tolerances regardless of the number of casualties.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-casualty-tolerance" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-casualty-tolerance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/casualty_tolerance.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-casualty-tolerance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Maximum acceptable annual risk levels for AI-caused casualties. Each point represents the median response for that casualty level, with error bars showing 95% confidence intervals. N=722 respondents (210 of 933 who passed attention checks selected ‚ÄòToo speculative to answer‚Äô), with casualty numbers randomised.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Note:</strong> <em>We included a consistency check which asked the same question with similar wording to confirm if people gave the same judgement. As per <a href="#appendix-i-risk-tolerance-among-consistency-validated-respondents">Appendix I</a>, 71% of people passed this consistency check, and findings were similar when only including these people.</em></p>
<p>However, we have less confidence in this question due to ambiguities introduced by the anchors on the risk ladder. These anchors were designed to help participants understand the numerical probabilities (e.g., 1 in 100,000). However, the societal anchors people received were the regulated risk tolerances for a <strong>single death</strong> in the <a href="https://www.sciencedirect.com/topics/engineering/maximum-tolerable-risk">workplace</a>, from a <a href="https://www.damsafety.nsw.gov.au/reporting">dam</a>, from a <a href="https://www.onr.org.uk/media/v1vi3v21/tolerability.pdf">nuclear plant</a>, or from <a href="https://www.faa.gov/documentLibrary/media/Advisory_Circular/AC_25.1309-1B.pdf">flying</a>. These were not scaled when the question asked about multiple deaths. For example, someone asked about the risk tolerance of 1 billion deaths was given the same risk anchors (e.g., ‚Äú0.0001% (0.1 in 100,000 chance) Similar to risk of death tolerated from commercial planes‚Äù) even though there is not a 0.0001% chance of aircraft causing 1 billion deaths. This introduced ambiguity in how the anchors could be interpreted, particularly for larger numbers.</p>
<p>Nevertheless, we report this item here for completeness as exploratory data. The data generally support findings from <a href="#australians-expect-ai-to-be-as-safe-as-commercial-aviation">simpler questions</a> showing low risk tolerances from AI.</p>
<hr>
</section>
<section id="appendix-i-risk-tolerance-among-consistency-validated-respondents" class="level2">
<h2 class="anchored" data-anchor-id="appendix-i-risk-tolerance-among-consistency-validated-respondents">Appendix I: Risk Tolerance Among Consistency-Validated Respondents</h2>
<p>To validate our risk tolerance findings, we re-analysed the randomised experiments using only participants who demonstrated response consistency. The consistency check asked participants to confirm their earlier risk tolerance: ‚ÄúEarlier you indicated {participant selected risk, e.g., 0.01%} was your maximum acceptable risk for {randomly selected number; e.g., 1,000} deaths from AI. To confirm, a technology with a {participant selected risk} chance of causing {casualties} deaths is: Far too risky Somewhat too risky About right Could accept somewhat higher risk Could accept much higher risk‚Äù We considered participants as ‚Äòinconsistent‚Äô when they felt their previous response was ‚Äòfar too risky‚Äô or said they ‚Äòcould accept much higher risk‚Äô.</p>
<section id="consistency-check-results" class="level3">
<h3 class="anchored" data-anchor-id="consistency-check-results">Consistency Check Results</h3>
<p>Of 932 participants, 663 (71.1%) passed the consistency check.</p>
</section>
<section id="casualty-risk-tolerance-consistency-validated" class="level3">
<h3 class="anchored" data-anchor-id="casualty-risk-tolerance-consistency-validated">Casualty Risk Tolerance (Consistency-Validated)</h3>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/casualty_tolerance_consistency.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Casualty risk tolerance among participants who passed consistency check</figcaption>
</figure>
</div>
</div>
</div>
<p>The pattern remains consistent: participants tolerate extremely low risks (~1 in 1,000,000) regardless of casualty scale, even when restricted to those who demonstrated response consistency.</p>
</section>
<section id="risk-benefit-trade-offs-consistency-validated" class="level3">
<h3 class="anchored" data-anchor-id="risk-benefit-trade-offs-consistency-validated">Risk-Benefit Trade-offs (Consistency-Validated)</h3>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/risk_benefit_tradeoff_consistency.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Risk-benefit trade-offs among consistent responders</figcaption>
</figure>
</div>
</div>
</div>
<p>Participants who passed the consistency check show similar extreme risk aversion, finding even small probabilities of catastrophe unacceptable despite major benefits.</p>
</section>
<section id="temporal-trade-offs-consistency-validated" class="level3">
<h3 class="anchored" data-anchor-id="temporal-trade-offs-consistency-validated">Temporal Trade-offs (Consistency-Validated)</h3>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/temporal_tradeoff_consistency.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Temporal trade-offs among consistent responders</figcaption>
</figure>
</div>
</div>
</div>
<p>The willingness to delay AI deployment for safety remains high among consistency-validated respondents.</p>
</section>
<section id="statistical-comparison" class="level3">
<h3 class="anchored" data-anchor-id="statistical-comparison">Statistical Comparison</h3>
<div class="cell" data-layout-align="center" data-tbl-cap="Comparison of risk tolerance measures between full and consistency-validated samples">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Comparison of risk tolerance measures between full and consistency-validated samples</caption>
<colgroup>
<col style="width: 23%">
<col style="width: 13%">
<col style="width: 20%">
<col style="width: 17%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Measure</th>
<th style="text-align: right;">Full Sample (n)</th>
<th style="text-align: left;">Full Sample (Mean ¬± SE)</th>
<th style="text-align: right;">Consistent Only (n)</th>
<th style="text-align: left;">Consistent Only (Mean ¬± SE)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Casualty Risk Tolerance</td>
<td style="text-align: right;">722</td>
<td style="text-align: left;">2.91 ¬± 0.07</td>
<td style="text-align: right;">663</td>
<td style="text-align: left;">2.88 ¬± 0.07</td>
</tr>
<tr class="even">
<td style="text-align: left;">Risk-Benefit Acceptability</td>
<td style="text-align: right;">830</td>
<td style="text-align: left;">2.31 ¬± 0.03</td>
<td style="text-align: right;">614</td>
<td style="text-align: left;">2.32 ¬± 0.04</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Temporal Delay Worthiness</td>
<td style="text-align: right;">932</td>
<td style="text-align: left;">4.17 ¬± 0.03</td>
<td style="text-align: right;">663</td>
<td style="text-align: left;">4.21 ¬± 0.04</td>
</tr>
</tbody>
</table>
</div>
</div>
<p><strong>Key Finding</strong>: The results are robust to excluding participants who failed the consistency check. The extreme risk aversion finding holds even when restricted to participants who demonstrated clear understanding and consistency in their responses.</p>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Only 19% cite lack of understanding<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>We used Claude Opus 4 to summarise those risks at a Year 7 reading age (full items in <a href="#fig-priority-full-appendix" class="quarto-xref">Figure&nbsp;19</a>), and shuffled the list for each participant.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>Unemployment is common and present; loss-of-control is speculative and unprecedented. Unemployment is debilitating but, in Australia, not life-threatening. In the worst instance, loss-of-control could mean the end of humanity<span class="citation" data-cites="danielkokotajlo2025"><sup><a href="#ref-danielkokotajlo2025" role="doc-biblioref">17</a></sup></span><a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>Our previous research suggests concrete images increases perceived risks<span class="citation" data-cites="ward2024"><sup><a href="#ref-ward2024" role="doc-biblioref">18</a></sup></span>. Without concrete explanations, it might not be obvious that ‚Äòloss of control‚Äô leads to any deaths. People could think it means an AI system doing something we don‚Äôt want for some short time, like purchasing the wrong size shoes.<a href="#fnref4" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p>We used Claude Opus 4 to describe these mitigations in Year 10 English (full labels in appendices as <a href="#appendix-a-survey-questions">list</a> and <a href="#appendix-b-full-label-plots">figure</a>) and shuffled the list for each participant.<a href="#fnref5" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn6"><p>‚ÄúWe prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker.‚Äù<span class="citation" data-cites="zhang2025"><sup><a href="#ref-zhang2025" role="doc-biblioref">26</a></sup></span> p.&nbsp;1<a href="#fnref6" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn7"><p>e.g., privacy laws, consumer protections, anti-discrimination legislation<a href="#fnref7" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn8"><p>‚ÄúScope neglect or scope insensitivity is a cognitive bias that occurs when the valuation of a problem is not valued with a multiplicative relationship to its size.‚Äù‚Äî<a href="https://en.wikipedia.org/wiki/Scope_neglect">Wikipedia</a><a href="#fnref8" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>