% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Lockey2020-ts,
  title     = "Trust in Artificial Intelligence: Australian Insights",
  author    = "Lockey, Steven and Gillespie, Nicole and Curtis, Caitlin",
  abstract  = "This report publishes the findings of the first Australian
               nation-wide survey looking at public attitude and trust in
               artificial intelligence (AI) systems. This national survey is
               the first to take a deep dive into answering this question and
               understanding community trust and expectations in relation to
               AI. To do this, we surveyed a nationally representative sample
               of over 2,500 Australian citizens in June to July 2020. Our
               findings provide important and timely research insights into the
               public's trust and attitudes towards AI and lay out a pathway
               for strengthening trust and acceptance of AI systems. Below, we
               summarise the key findings. In the conclusion to the report, we
               draw out the implications of these insights for government,
               business and NGOs. Abstract/Description Artificial Intelligence
               (AI) is the cornerstone technology of the Fourth Industrial
               Revolution and is enabling rapid innovation with many potential
               benefits for Australian society (e.g. enhanced healthcare
               diagnostics, transportation optimisation) and business (e.g.
               enhanced efficiency and competitiveness). The COVID-19 pandemic
               has accelerated the uptake of advanced technology, and
               investment in AI continues to grow exponentially. AI also poses
               considerable risks and challenges to society which raises
               concerns about whether AI systems are worthy of trust. These
               concerns have been fuelled by high profile cases of AI use that
               were biased, discriminatory, manipulative, unlawful, or violated
               privacy or other human rights. Without public confidence that AI
               is being developed and used in an ethical and trustworthy
               manner, it will not be trusted and its full potential will not
               be realised. To echo the sentiment of Dr Alan Finkel AO,
               Australia's Chief Scientist, acceptance of AI rests on ``the
               essential foundation of trust''. Are we capable of extending our
               trust to AI? This national survey is the first to take a deep
               dive into answering this question and understanding community
               trust and expectations in relation to AI. To do this, we
               surveyed a nationally representative sample of over 2,500
               Australian citizens in June to July 2020. Our findings provide
               important and timely research insights into the public's trust
               and attitudes towards AI and lay out a pathway for strengthening
               trust and acceptance of AI systems. Key findings include: -
               Trust is central to the acceptance of AI, and is influenced by
               four key drivers; - Australians have low trust in AI systems but
               generally `accept' or `tolerate' AI; - Australians expect AI to
               be regulated and carefully managed; - Australians expect
               organisations to uphold the principles of trustworthy AI; -
               Australians feel comfortable with some but not all uses of AI at
               work; - Australians want to know more about AI but currently
               have low awareness and understanding of AI and its uses. We draw
               out the implications of the findings for government, business
               and NGOs and provide a roadmap to enhancing public trust in AI
               highlighting three key actions: - Live up to Australian's
               expectations of trustworthy AI - Strengthen the regulatory
               framework for governing AI - Strengthen Australia's AI literacy",
  publisher = "The University of Queensland and KPMG Australia",
  month     =  oct,
  year      =  2020
}

@TECHREPORT{Elsey2023-ik,
  title       = "{US} public opinion of {AI} policy and risk",
  author      = "Elsey, Jamie and Moss, David",
  institution = "Rethink Priorities",
  month       =  may,
  year        =  2023
}

@TECHREPORT{noauthor_2023-dy,
  title       = "{YouGov} Survey: {AI} and the End of Humanity",
  institution = "YouGov",
  year        =  2023
}

@TECHREPORT{noauthor_2023-sk,
  title       = "The {AI} Policy Institute ({AIPI)/YouGov} survey of {US}
                 voters about {AI}",
  institution = "The AI Policy Institute and YouGov",
  year        =  2023
}

@TECHREPORT{Selwyn2020-rk,
  title     = "{AI} for social good - Australian attitudes toward {AI} and
               society report.Pdf",
  author    = "Selwyn, Neil and Cordoba, Beatriz Gallo and Andrejevic, Mark and
               Campbell, Liz",
  abstract  = "Based on a nationally-representative public opinion survey of
               over 2000 Australian adults, the report examines key areas of
               public understanding, optimism and concern regarding the
               societal application of AI technologies. As industry and
               policymakers continue to develop, implement and manage AI across
               most areas of Australian society, this report explores the often
               overlooked views of the general public -- in many ways, the
               ultimate `end users' of these powerful technologies.",
  publisher = "Monash University",
  month     =  oct,
  year      =  2020
}

@TECHREPORT{Karger2023-to,
  title       = "Forecasting Existential Risks, Evidence from a {Long-Run}
                 Forecasting Tournament",
  author      = "Karger, Ezra and Rosenberg, Josh and Jacobs, Zachary and
                 Hickman, Molly and Hadshare, Rose and Gamin, Kayla and Smith,
                 Taylor and Williams, Bridget and McCaslin, Tegan and Thomas,
                 Stephen and Tetlock, Philip E",
  institution = "Forecasting Research Institute",
  year        =  2023
}

@TECHREPORT{YouGov2023-zx,
  title       = "{YouGov} Survey: Identifying {AI}",
  author      = "{YouGov}",
  institution = "YouGov",
  year        =  2023
}

@TECHREPORT{Davidson2023-kw,
  title       = "What a compute-centric framework says about takeoff speeds",
  author      = "Davidson, Tom",
  abstract    = "In the next few decades we may develop AI that can automate
                 ~all cognitive tasks and dramatically transform the world. By
                 contrast, today the capabilities and impact of AI are much
                 more limited. Once we have AI that could readily automate 20\%
                 of cognitive tasks (weighted by 2020 economic value), how much
                 longer until it can automate 100\%? This is what I refer to as
                 the question of AI takeoff speeds; this report develops a
                 compute-centric framework for answering it. First, I estimate
                 how much more ``effective compute'' -- a measure that combines
                 compute with the quality of AI algorithms -- is needed to
                 train AI that could readily perform 100\% of tasks compared to
                 AI that could just perform 20\% of tasks; my best-guess is 4
                 orders of magnitude more (i.e. 10,000X as much). Then, using a
                 computational semi-endogenous growth model, I simulate how
                 long it will take for the effective compute used in the
                 largest training run to increase by this amount: the model's
                 median prediction is just 3 years. The simulation models the
                 effect of both rising human investments and increasing AI
                 automation on AI R\&D progress.",
  institution = "Open Philanthropy",
  month       =  jun,
  year        =  2023,
  language    = "en"
}

@TECHREPORT{Grace2022-iv,
  title       = "2022 Expert Survey on Progress in {AI}",
  author      = "Grace, Katja and Stein-Perlman, Zach and Weinstein-Raun, Ben
                 and Salvatier, John",
  abstract    = "Collected data and analysis from a large survey of machine
                 learning researchers.",
  institution = "AI Impacts",
  month       =  aug,
  year        =  2022,
  language    = "en"
}

@ARTICLE{Anderljung2023-cj,
  title         = "Frontier {AI} Regulation: Managing Emerging Risks to Public
                   Safety",
  author        = "Anderljung, Markus and Barnhart, Joslyn and Korinek, Anton
                   and Leung, Jade and O'Keefe, Cullen and Whittlestone, Jess
                   and Avin, Shahar and Brundage, Miles and Bullock, Justin and
                   Cass-Beggs, Duncan and Chang, Ben and Collins, Tantum and
                   Fist, Tim and Hadfield, Gillian and Hayes, Alan and Ho,
                   Lewis and Hooker, Sara and Horvitz, Eric and Kolt, Noam and
                   Schuett, Jonas and Shavit, Yonadav and Siddarth, Divya and
                   Trager, Robert and Wolf, Kevin",
  abstract      = "Advanced AI models hold the promise of tremendous benefits
                   for humanity, but society needs to proactively manage the
                   accompanying risks. In this paper, we focus on what we term
                   ``frontier AI'' models: highly capable foundation models
                   that could possess dangerous capabilities sufficient to pose
                   severe risks to public safety. Frontier AI models pose a
                   distinct regulatory challenge: dangerous capabilities can
                   arise unexpectedly; it is difficult to robustly prevent a
                   deployed model from being misused; and, it is difficult to
                   stop a model's capabilities from proliferating broadly. To
                   address these challenges, at least three building blocks for
                   the regulation of frontier models are needed: (1)
                   standard-setting processes to identify appropriate
                   requirements for frontier AI developers, (2) registration
                   and reporting requirements to provide regulators with
                   visibility into frontier AI development processes, and (3)
                   mechanisms to ensure compliance with safety standards for
                   the development and deployment of frontier AI models.
                   Industry self-regulation is an important first step.
                   However, wider societal discussions and government
                   intervention will be needed to create standards and to
                   ensure compliance with them. We consider several options to
                   this end, including granting enforcement powers to
                   supervisory authorities and licensure regimes for frontier
                   AI models. Finally, we propose an initial set of safety
                   standards. These include conducting pre-deployment risk
                   assessments; external scrutiny of model behavior; using risk
                   assessments to inform deployment decisions; and monitoring
                   and responding to new information about model capabilities
                   and uses post-deployment. We hope this discussion
                   contributes to the broader conversation on how to balance
                   public safety risks and innovation benefits from advances at
                   the frontier of AI development.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2307.03718"
}

@UNPUBLISHED{Weil2024-wm,
  title    = "Tort Law as a Tool for Mitigating Catastrophic Risk from
              Artificial Intelligence",
  author   = "Weil, Gabriel",
  abstract = "The capabilities of artificial intelligence (AI) systems have
              improved markedly over the past decade. This rapid progress has
              brought greater attention to longstanding concerns that advanced
              AI systems could cause catastrophic harm, up to and including
              human extinction. In principle, the prospect of tort liability
              could encourage AI developers to proceed with caution. But the
              current U.S. tort liability system is not set up to handle the
              catastrophic risk posed by AI --- since most of the expected harm
              from AI systems comes in truly catastrophic scenarios where
              compensation would not be feasible. To address this, I propose a
              form of punitive damages designed to pull forward this expected
              liability into cases of practically compensable harm. To succeed
              in offering sufficient incentives for precaution, such punitive
              damages would need to be available even in the absence of human
              malice or recklessness. This paper also examines other legal
              mechanisms to ensure socially optimal investment in AI safety
              measures. Doctrinal changes considered include recognizing the
              training and deployment of advanced AI systems as an abnormally
              dangerous activity subject to strict liability, adopting a
              capacious concept of foreseeability for the purposes of
              evaluating proximate cause in cases of AI harm, and modifying the
              scope of legally compensable damages in cases involving loss of
              human life. The paper also discusses further AI liability law
              changes that would require legislation, including requiring
              liability insurance for training and deployment of advanced AI
              systems, diverting a portion of punitive damages into an AI
              safety fund, and credibly announcing the availability of strict
              liability and punitive damages in advance of the first case in
              which they might be appropriate. The paper concludes by
              characterizing the limits of an ex post tort liability framework
              for limiting catastrophic AI risk and considers what
              complementary policies may be needed to plug the gaps left by
              such a framework.",
  month    =  jan,
  year     =  2024,
  keywords = "AI, artificial intelligence; torts; existential risk,
              catastrophic risk, law and economics, externality, strict
              liability, punitive damages, liability insurance, AI governance"
}

@MISC{Dafoe2018-la,
  title        = "{AI} governance: Opportunity and theory of impact",
  author       = "Dafoe, Allan",
  abstract     = "AI governance concerns how humanity can best navigate the
                  transition to a world with advanced AI systems[1]. It relates
                  to how decisions are made about AI[2], and what institutions
                  and arrangements would help those decisions to be made well.
                  I believe advances in AI are likely to be among the",
  publisher    = "Centre for the Governance of AI",
  year         =  2018,
  howpublished = "\url{https://www.allandafoe.com/opportunity}",
  note         = "Accessed: 2024-2-28",
  language     = "en"
}

@MISC{Center_for_AI_Safety2023-bj,
  title        = "Statement on {AI} risk",
  author       = "{Center for AI Safety}",
  abstract     = "A statement jointly signed by a historic coalition of
                  experts: ``Mitigating the risk of extinction from AI should
                  be a global priority alongside other societal-scale risks
                  such as pandemics and nuclear war.''",
  year         =  2023,
  howpublished = "\url{https://www.safe.ai/statement-on-ai-risk}",
  note         = "Accessed: 2024-2-28",
  language     = "en"
}

@ARTICLE{Dafoe2018-ih,
  title   = "{AI} Governance: A Research Agenda",
  author  = "Dafoe, Allan",
  journal = "Centre for the Governance of AI",
  year    =  2018
}

@ARTICLE{Shevlane2023-uz,
  title         = "Model evaluation for extreme risks",
  author        = "Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben
                   and Phuong, Mary and Whittlestone, Jess and Leung, Jade and
                   Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus
                   and Kolt, Noam and Ho, Lewis and Siddarth, Divya and Avin,
                   Shahar and Hawkins, Will and Kim, Been and Gabriel, Iason
                   and Bolina, Vijay and Clark, Jack and Bengio, Yoshua and
                   Christiano, Paul and Dafoe, Allan",
  abstract      = "Current approaches to building general-purpose AI systems
                   tend to produce systems with both beneficial and harmful
                   capabilities. Further progress in AI development could lead
                   to capabilities that pose extreme risks, such as offensive
                   cyber capabilities or strong manipulation skills. We explain
                   why model evaluation is critical for addressing extreme
                   risks. Developers must be able to identify dangerous
                   capabilities (through ``dangerous capability evaluations'')
                   and the propensity of models to apply their capabilities for
                   harm (through ``alignment evaluations''). These evaluations
                   will become critical for keeping policymakers and other
                   stakeholders informed, and for making responsible decisions
                   about model training, deployment, and security.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2305.15324"
}

@TECHREPORT{Nestor_Maslej_Loredana_Fattorini_Erik_Brynjolfsson_John_Etchemendy_Katrina_Ligett_Terah_Lyons_James_Manyika_Helen_Ngo_Juan_Carlos_Niebles_Vanessa_Parli_Yoav_Shoham_Russell_Wald_Jack_Clark_and_Raymond_Perrault2023-vp,
  title       = "The {AI} Index 2023 Annual Report",
  author      = "{Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John
                 Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen
                 Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell
                 Wald, Jack Clark, and Raymond Perrault}",
  institution = "Stanford University",
  month       =  apr,
  year        =  2023
}

@TECHREPORT{noauthor_2024-mp,
  title       = "Safe and responsible {AI} in Australia consultation:
                 Australian Government's interim response",
  institution = "Department of Industry, Science, and Resources",
  month       =  jan,
  year        =  2024
}

@ARTICLE{Grace2017-ko,
  title         = "When Will {AI} Exceed Human Performance? Evidence from {AI}
                   Experts",
  author        = "Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang,
                   Baobao and Evans, Owain",
  abstract      = "Advances in artificial intelligence (AI) will transform
                   modern life by reshaping transportation, health, science,
                   finance, and the military. To adapt public policy, we need
                   to better anticipate these advances. Here we report the
                   results from a large survey of machine learning researchers
                   on their beliefs about progress in AI. Researchers predict
                   AI will outperform humans in many activities in the next ten
                   years, such as translating languages (by 2024), writing
                   high-school essays (by 2026), driving a truck (by 2027),
                   working in retail (by 2031), writing a bestselling book (by
                   2049), and working as a surgeon (by 2053). Researchers
                   believe there is a 50\% chance of AI outperforming humans in
                   all tasks in 45 years and of automating all human jobs in
                   120 years, with Asian respondents expecting these dates much
                   sooner than North Americans. These results will inform
                   discussion amongst researchers and policymakers about
                   anticipating and managing trends in AI.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1705.08807"
}

@TECHREPORT{Gillespie2023-ag,
  title     = "Trust in Artificial Intelligence: A global study",
  author    = "Gillespie, Nicole and Lockey, Steven and Curtis, Caitlin and
               Pool, Javad and Akbari, Ali",
  publisher = "The University of Queensland; KPMG Australia",
  month     =  feb,
  year      =  2023,
  address   = "Brisbane, Australia"
}

@ARTICLE{Maslej2023-bz,
  title         = "Artificial Intelligence Index Report 2023",
  author        = "Maslej, Nestor and Fattorini, Loredana and Brynjolfsson,
                   Erik and Etchemendy, John and Ligett, Katrina and Lyons,
                   Terah and Manyika, James and Ngo, Helen and Niebles, Juan
                   Carlos and Parli, Vanessa and Shoham, Yoav and Wald, Russell
                   and Clark, Jack and Perrault, Raymond",
  abstract      = "Welcome to the sixth edition of the AI Index Report. This
                   year, the report introduces more original data than any
                   previous edition, including a new chapter on AI public
                   opinion, a more thorough technical performance chapter,
                   original analysis about large language and multimodal
                   models, detailed trends in global AI legislation records, a
                   study of the environmental impact of AI systems, and more.
                   The AI Index Report tracks, collates, distills, and
                   visualizes data related to artificial intelligence. Our
                   mission is to provide unbiased, rigorously vetted, broadly
                   sourced data in order for policymakers, researchers,
                   executives, journalists, and the general public to develop a
                   more thorough and nuanced understanding of the complex field
                   of AI. The report aims to be the world's most credible and
                   authoritative source for data and insights about AI.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2310.03715"
}

@MISC{noauthor_2023-kp,
  title        = "Pause Giant {AI} Experiments: An Open Letter",
  booktitle    = "Future of Life Institute",
  abstract     = "We call on all AI labs to immediately pause for at least 6
                  months the training of AI systems more powerful than GPT-4.",
  month        =  mar,
  year         =  2023,
  howpublished = "\url{https://futureoflife.org/open-letter/pause-giant-ai-experiments/}",
  note         = "Accessed: 2024-3-7",
  language     = "en"
}
