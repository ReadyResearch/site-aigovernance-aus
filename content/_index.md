---
title:
date: 2025-01-01
type: landing

sections:
  - block: hero
    content:
      title: |-
        Survey Assessing Risks from AI
      text: |-
        Understanding Australian public perceptions of AI risks and support for AI governance.
        <br>
        **SARA** surveys Australian adults about their concerns regarding AI risks, support for AI development and regulation, and priority governance actions to address these risks.
        <br>
      cta:
        label: Explore 2025 Survey
        url: '/survey/2025/'
        icon_pack: fas
        icon: star
      cta_alt:
        label: Explore 2024 Survey
        url: '/survey/2024/'
    design:
      background:
        image:
          filename: dominic-kurniawan-suryaputra-8bA_OXkFq2s-unsplash.jpg
          filters:
            brightness: 0.5
        text_color_light: true

  - block: markdown
    content:
      title: About SARA
      text: |-
        The **Survey Assessing Risks from AI (SARA)** is an annual representative survey of Australian adults investigating:
        - Public perceptions of AI risks (from current harms to potential catastrophic risks)
        - Support for AI development and regulation
        - Priority governance actions to address AI risks

        SARA generates 'evidence for action' to help public and private actors make informed decisions about safer AI development and use.

        This project is a collaboration between Ready Research and The University of Queensland.

    design:
      columns: '2'

  - block: markdown
    content:
      title: Latest Findings
      text: |-
        **2025 Survey** (933 Australians):

        - Australians expect AI to be as safe as commercial aviation—at least **4,000 times safer** than current risk estimates
        - There is strong public demand for government to better manage AI risks
        - Many proposed risk controls would increase public trust in AI

        [Explore the full 2025 survey findings →](/survey/2025/)

    design:
      columns: '2'

  - block: markdown
    content:
      title: What This Means for Australia
      text: |-
        These findings reveal a clear public mandate for stronger AI governance in Australia. Australians expect the same rigorous safety standards for AI that we apply to aviation and other critical technologies.

        The research shows that:
        - **Public expectations for AI safety are high**: Australians want AI systems to meet world-class safety standards
        - **Government action is needed**: There is broad support for regulatory intervention to manage AI risks
        - **Trust can be built**: Implementing appropriate safety controls would increase public confidence in AI technology

        This evidence base can inform policy decisions, regulatory frameworks, and governance approaches for AI in Australia.

    design:
      columns: '2'

  - block: markdown
    content:
      title: AI Governance
      text: |-
        **AI governance** encompasses the norms, policies, laws, processes, and institutions that guide responsible decision-making about AI development, deployment, and use. Effective governance is crucial for managing both current harms and potential catastrophic risks from AI, including risks from misuse, accidents, or loss of control.

        ### Australian Organizations

        **[Good Ancestors Policy](https://goodancestors.org.au)** — A policy advocacy organisation focused on AI safety and governance to ensure beneficial outcomes for future generations.

        **[Tech Policy Design Institute](https://techpolicy.au)** — An independent Australian institute conducting research and advocacy on technology policy, including AI governance.

        **[Human Technology Institute](https://www.uts.edu.au/hti)** — Based at the University of Technology Sydney, this institute conducts applied research and consulting to support corporate and government decision-making about AI.

        **[Centre for AI and Digital Ethics](https://law.unimelb.edu.au/centres/caide)** — Based at the University of Melbourne, this centre researches ethical, technical, regulatory, and legal issues relevant to AI.

        **[Gradient Institute](https://gradientinstitute.org)** — A Sydney-based independent institute conducting applied research and consulting to improve the safety, ethics, accountability, and transparency of AI systems.

        ### International Organizations

        **[Centre for the Governance of AI](https://www.governance.ai)** — This organization conducts and convenes research dedicated to helping humanity navigate the transition to a world with advanced AI.

        **[BlueDot Impact](https://bluedot.org)** — Offers cohort-based courses including AI Governance, covering the policy landscape, regulatory tools, and institutional reforms needed for beneficial AI outcomes.

        **[The Institute for AI Policy and Strategy](https://www.iaps.ai)** — This organization focuses on understanding and managing risks from advanced AI systems, with emphasis on AI policy and standards, compute governance, and international governance.

    design:
      columns: '2'

  - block: contact
    id: contact
    content:
      title: Contact
      subtitle: ''
      text: |-
        Contact [Dr Alexander Saeri](mailto:a.saeri@uq.edu.au) to discuss the research project and its findings.

        <br>
        <br>
        <img src='/readyuqlogo.png' alt='Ready Research & The University of Queensland logo' style='display: block; margin-right: auto; margin-left: auto;width: 60%' />
      autolink: true
      design:
        columns: '2'
---
